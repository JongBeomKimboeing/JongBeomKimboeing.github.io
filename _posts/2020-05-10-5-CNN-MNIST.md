---
layout: post
title: MNIST with CNN
description: "MNIST with CNN"
modified: 2020-05-10
tags: [김성훈,DL]
categories: [김성훈DL]
---
# convolutional Neural Network를 이용한 MNIST classification
```python
import tensorflow as tf
import numpy as np
import random

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train = x_train / 225
x_test = x_test / 225
print(x_train.shape) #(60000, 28, 28)
x_train = x_train.reshape(x_train.shape[0] ,28 ,28 ,1) # (60000, 28, 28, 1) #6000개의 데이터를 28*28의 이미지로 변환하고, 색은 하나이다.
print(x_train.shape)
print(x_test.shape) #(10000, 28, 28)
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)
print(x_test.shape) # (10000, 28, 28, 1)

y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)
y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)

learning_rate = 0.001
training_epoch = 12
batch_size = 128

#-----------------------------------------------------------------------------------------------------------
#layer1
tf.model = tf.keras.Sequential()
tf.model.add(tf.keras.layers.Conv2D(filters= 16, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))

# tf.keras.layers.Conv2D-> filters= : fiter의 개수, kernel_size=: kernal 크기,  input_shape=: input data의 모양
#                       -> strides=(1, 1): filter를 얼마만큼의 stride로 움직일 것인가.

tf.model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))

#layer2
tf.model.add(tf.keras.layers.Conv2D(filters= 32, kernel_size=(3,3), activation='relu'))
tf.model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))

#layer3(Fully connected)
tf.model.add(tf.keras.layers.Flatten())
tf.model.add(tf.keras.layers.Dense(units=10, kernel_initializer='glorot_normal', activation='softmax'))
tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=['accuracy'])
tf.model.summary()
#-----------------------------------------------------------------------------------------------------------

tf.model.fit(x_train,y_train,batch_size=batch_size,epochs=training_epoch)

y_predicted = tf.model.predict(x_test)
for x in range(0, 10):
    random_index = random.randint(0, x_test.shape[0]-1)
    print("index: ", random_index,
          "actual y: ", np.argmax(y_test[random_index]),
          "predicted y: ", np.argmax(y_predicted[random_index]))

evaluation = tf.model.evaluate(x_test, y_test)
print('loss: ', evaluation[0])
print('accuracy', evaluation[1])
```
## 결과
### training set에 대한 loss와 accuracy
```python
loss: 0.0282 - accuracy: 0.9911
```

### 10개의 random한 test set에 대한 label과 prediction 비교
```python
index:  7223 actual y:  1 predicted y:  1
index:  5105 actual y:  2 predicted y:  2
index:  9035 actual y:  5 predicted y:  5
index:  2290 actual y:  7 predicted y:  7
index:  9862 actual y:  6 predicted y:  6
index:  9439 actual y:  0 predicted y:  0
index:  3979 actual y:  6 predicted y:  6
index:  3687 actual y:  9 predicted y:  9
index:  412 actual y:  5 predicted y:  5
index:  4711 actual y:  5 predicted y:  5
```

### test set에 대한 loss와 accuracy
```python
loss:  0.0410539047746046
accuracy 0.9868
```
## 고찰
이번 실습에서는 2단 convolutional layer와 maxpool을 사용하였고,<br>
단일 softmax layer를 통과시켜 위 결과를 얻게 됐다.<br>
training set과 test set의 결과를 보아 DropOut을 사용하여 overfitting을 줄이고,<br>
weight initializer를 이용하여 더 높은 accuracy를 내야겠다.
