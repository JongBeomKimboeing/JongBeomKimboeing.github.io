---
layout: post
title: Machine Learning Basis 5
description: "Machine Learning Basis"
modified: 2020-06-28
tags: [Machine Learning]
categories: [Machine Learning]
---

# tensorflow 기초 사용법
<br>

## 1. 상수형 텐서 선언 -> 절대 변하지 않는 값

```python
import tensorflow as tf

tensor_a = tf.constant(value, dtype=None, shape= None, name=None)
```

<br>
<br>

## 2. 모든 원소 값이 0인 tensor 생성

```python
import tensorflow as tf

tensor_b = tf.zeros(shape, dtype=tf.float32, name=None)
-> 데이터 초기화할 때 사용
-> shape에는 튜플을 넣어줘야함 ex) (1,2)=1행2열
```
<br>
<br>

## 3. 모든 원소 값이 1인 tensor 생성

```python
import tensorflow as tf

tensor_c = tf.ones(shape, dtype=tf.float32, name=None)
-> 데이터 초기화할 때 사용
-> shape에는 튜플을 넣어줘야함 ex) (1,2)=1행2열
```
<br>
<br>

## 4. 시퀀스 선언하기

```python
import tensorflow as tf

tesor_d = tf.linspace(start=, stop=, num=,name=)

'''
start: 시작 값
stop: 끝 값
num: 생성할 데이터 수
name: 시퀀스 이름
-> 시작 값 부터 끝 값까지 num의 개수만큼 생성
'''
```

<br>
<br>

## 5. start에서 stop까지 delta씩 증가하는 데이터

```python
import tensorflow as tf

tensor_e = tf.range(start=,limit=,delta=,name=)

'''
start: 시작값
limit: 끝 값
delta: 증가량
name: 시퀀스 이름
'''
```

<br>
<br>

## 6. 난수 선언하기
-> 난수를 통해 tensor를 초기화해주는 경우가 많고, 난수로 초기화 시켜주면 성능이 더 높게 나오기도 한다.<br>

### 1) 정규분포 생성

```python
# 정규분포 생성
tensor_f = tf.random.normal(shape=, mean=, stddev=, dtype=, seed=, name=)
->mean: 평균 (주로 0으로 지정)
->stddev: 표준편차(주로 1로 지정)
```

### 2) 균등분포 생성

```python
# 균등분포 생성
tf.random.uniform(shape=, minval=, maxval=, dtype=, seed=, name=)
-> -1과 1사이의 값을 균등한 확률로 random하게 값을 지정하고 싶은 경우
(정규분포는 정규분포 그래프를 참고하면, 0 근처의 값이 나올 확률이 높을 수 있다는 것을 볼 수 있다.)
```

<br>
<br>

## 7. 변수선언

```python
tensor_f = tf.Variable(value, name=)

# 일반적인 퍼셉트론의 가중치와 bias 생성
weight = tf.Variable(10)
bias = tf.Variable(tf.random.normal([10,10])) # 정규분포로 초기화
```

<br>
<br>

## 8. 연산자

```python
tf.negative(x) # -x (숫자만)
tf.logical_not(x) # !x (boolean 값만)
tf.abs(x) # x의 절대값 (숫자만)

tf.add(x,y) # x + y
tf.subtract(x,y) # x - y
tf.multiply(x,y) # x * y
tf.truediv(x,y) # x / y
tf.math.mod(x,y) # x % y
tf.math.pow(x,y) # x ** y
```

<br>
<br>

## 9. 사용 예시


### ex1) tensor 생성 

```python
def constant_tensors():
    # 5의 값을 가지는 (1,1) shape의 8-bit integer 텐서를 만드세요.
    t1 = tf.constant(value=5, shape=(1,1),dtype=tf.int8)

    # 모든 원소의 값이 0인 (3,5) shape의 16-bit integer 텐서를 만드세요.
    t2 = tf.constant(value=0, shape=(3,5),dtype=tf.int16)
    # 모든 원소의 값이 1인 (4,3) shape의 8-bit integer 텐서를 만드세요.
    t3 = tf.constant(value=1, shape=(4,3),dtype=tf.int8)

    return t1, t2, t3


def sequence_tensors():
    # 1.5에서 10.5까지 증가하는 3개의 텐서를 만드세요.
    seq_t1 = tf.linspace(start=1.5, stop=10.5, num=3,name=None)

    # 1에서 10까지 2씩 증가하는 텐서를 만드세요.
    seq_t2 = tf.range(start=1, limit=10, delta=2,name=None)

    return seq_t1, seq_t2


def random_tensors():
    # 난수를 생성하기 위한 seed 값입니다.
    # 정확한 채점을 위해 값을 변경하지 마세요!
    seed = 3921

    # 평균이 0이고 표준편차가 1인  정규 분포를 가진 (7,4) shape의 32-bit float 난수 텐서를 만드세요.
    # 정확한 채점을 위하여 미리 설정된 seed 값을 사용해주세요.
    rand_t1 = tf.random.normal((7,4),mean=0,stddev=1,dtype=tf.float32,seed=seed)

    # 최소값이 0이고 최대값이 3인 균등 분포를 가진 (5,4,3) shape의 32-bit float 난수 텐서를 만드세요.
    # 정확한 채점을 위하여 미리 설정된 seed 값을 사용해주세요.
    rand_t2 = tf.random.uniform((5,4,3), minval=0, maxval=3,dtype=tf.float32,seed=seed)

    return rand_t1, rand_t2


def variable_tensor():
    # 값이 100인 변수 텐서를 만드세요.
    var_tensor = tf.Variable(initial_value=100)

    return var_tensor


def main():
    # 1. constant_tensors 함수를 완성하세요.
    t1, t2, t3 = constant_tensors()

    # 2. sequence_tensors 함수를 완성하세요.
    seq_t1, seq_t2 = sequence_tensors()

    # 3. random_tensors 함수를 완성하세요.
    rand_t1, rand_t2 = random_tensors()

    # 4. variable_tensor 함수를 완성하세요.
    var_tensor = variable_tensor()

    for i in [t1, t2, t3, seq_t1, seq_t2, rand_t1, rand_t2, var_tensor]:
        print(i.numpy())


if __name__ == "__main__":
    main()
```

<br>
<br>

### ex2) tensor 연산 예제

```python
import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

a = tf.constant(10, dtype = tf.int32)
b = tf.constant(3, dtype = tf.int32)
c = tf.constant(-10, dtype = tf.int32)
boolean = tf.constant(True, dtype = tf.bool)

# 1. 단항 연산자를 사용해보세요.
neg = tf.negative(a)
logic = tf.logical_not(boolean)
absolute = tf.abs(c)

# 2. 이항 연산자를 사용해 사칙연산을 수행해보세요.
add = tf.add(a,b)
sub = tf.subtract(a,b)
mul = tf.multiply(a,b)
div = tf.truediv(a,b)

for i in [neg, logic, absolute, add, sub, mul, div]:
    print(i.numpy())
```

<br>
<br>

### ex3) 계산기 만들기

```python
import tensorflow as tf
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


# 1. 두 실수와 연산 종류를 입력받는 함수입니다. 코드를 살펴보세요.
def insert():
    x = float(input('정수 또는 실수를 입력하세요. x : '))
    y = float(input('정수 또는 실수를 입력하세요. y : '))
    cal = input('어떤 연산을 할것인지 입력하세요. (+, -, *, /)')
    return x, y, cal


# 사칙연산 함수를 구현해보세요.
def calcul(x, y, cal):
    result = 0
    if cal == '+':

    # 더하기
        result = tf.add(x,y)

    # 빼기
    if cal == '-':
        result = tf.subtract(x,y)
    # 곱하기
    if cal == '*':
        result = tf.multiply(x,y)
    # 나누기
    if cal == '/':
        result = tf.truediv(x,y)
    return result.numpy() # numpy형식으로 반드시 변환시켜줘야한다.


def main():
    # 두 실수와 연산 종류를 입력받는 insert 함수를 호출합니다.
    x, y, cal = insert()

    # calcul 함수를 호출해 실수 사칙연산을 수행하고 결과를 출력해보세요.
    print(calcul(x, y, cal))


if __name__ == "__main__":
    main()
```

<br>
<br>
<br>
<br>

# 선형회귀 구현

## 1. 선형회귀 구현 코드

```python
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# 채점을 위해 랜덤 시드를 고정하는 코드입니다.
# 정확한 채점을 위해 코드를 변경하지 마세요!
np.random.seed(100)


# 선형 회귀 클래스 구현
class LinearModel:
    def __init__(self):
        # 1. 가중치 초기값을 1.5의 값을 가진 변수 텐서로 설정하세요.
        self.W = tf.Variable(1.5)

        # 1. 편향 초기값을 1.5의 값을 가진 변수 텐서로 설정하세요.
        self.b = tf.Variable(1.5)

    def __call__(self, X, Y):
        # 2. W, X, b를 사용해 선형 모델을 구현하세요.
        Y = tf.add(tf.multiply(X, self.W),self.b)
        return Y


# 3. MSE 값으로 정의된 loss 함수 선언
def loss(y, pred):
    return tf.reduce_mean(tf.square(y-pred))


# gradient descent 방식으로 학습 함수 선언
def train(linear_model, x, y):
    with tf.GradientTape() as t:
        current_loss = loss(y, linear_model(x, y))

    # learning_rate 값 선언
    learning_rate = 0.001

    # gradient 값 계산
    delta_W, delta_b = t.gradient(current_loss, [linear_model.W, linear_model.b])

    # learning rate와 계산한 gradient 값을 이용하여 업데이트할 파라미터 변화 값 계산
    W_update = (learning_rate * delta_W)
    b_update = (learning_rate * delta_b)

    return W_update, b_update


def main():
    # 데이터 생성
    x_data = np.linspace(0, 10, 50)
    y_data = 4 * x_data + np.random.randn(*x_data.shape) * 4 + 3

    # 데이터 출력
    plt.scatter(x_data, y_data)
    plt.savefig('data.png')
    plt.show()

    # 선형 함수 적용
    linear_model = LinearModel()

    # epochs 값 선언
    epochs = 100

    # epoch 값만큼 모델 학습
    for epoch_count in range(epochs):

        # 선형 모델의 예측 값 저장
        y_pred_data = linear_model(x_data, y_data)

        # 예측 값과 실제 데이터 값과의 loss 함수 값 저장
        real_loss = loss(y_data, linear_model(x_data, y_data))

        # 현재의 선형 모델을 사용하여  loss 값을 줄이는 새로운 파라미터로 갱신할 파라미터 변화 값을 계산
        update_W, update_b = train(linear_model, x_data, y_data)

        # 선형 모델의 가중치와 편향을 업데이트합니다.
        linear_model.W.assign_sub(update_W)
        # linear_model.W -= update_W
        linear_model.b.assign_sub(update_b)
        # linear_model.b -= update_W

        # 20번 마다 출력 (조건문 변경 가능)
        if (epoch_count % 20 == 0):
            print(f"Epoch count {epoch_count}: Loss value: {real_loss.numpy()}")
            print('W: {}, b: {}'.format(linear_model.W.numpy(), linear_model.b.numpy()))

            fig = plt.figure()
            ax1 = fig.add_subplot(111)
            ax1.scatter(x_data, y_data)
            ax1.plot(x_data, y_pred_data, color='red')
            plt.savefig('prediction.png')
            plt.show()

if __name__ == "__main__":
    main()
```

<br>
<br>
<br>
<br>

# 딥러닝 구현하기

## 1. epoch와 batch

epoch: 한 번의 epoch는 전체 데이터 셋에 대해 한 번 학습을 완료한 상태 <br>
<br>
batch: batch(보통 mini-batch라고 표현)는 나눠진 데이터 셋을 뜻하며 <br>
       iteration은 epoch를 나누어서 실행하는 횟수를 의미한다.<br>
       <br>
ex) 총 데이터가 1000개, batch size=100<br>
    1 iteration = 100개 데이터에 대해서 학습<br>
    1 epoch = 1000개의 데이터를 모두 봄<br>
    iteration = 1000/100 = 10<br>
































































































































