---
layout: post
title: Machine Learning Basis 1
description: "Machine Learning Basis"
modified: 2020-06-28
tags: [Machine Learning]
categories: [Machine Learning]
---

# Linear Algebra basis

<br>
<br>

## 1. 스칼라와 벡터
- 스칼라: 방향이 없고 크기만 존재하는 양
- 벡터: 방향과 크기가 존재하는 양

<br>
<br>
<br>

## 2. 벡터 공간 / 내적
<br>

### 1) Norm

Norm: n차원 벡터 x = (x1, x2, ... ,x3)에 대해<br>
Norm ||x|| = root(x1^2 + x2^2 + ... +xn^2)이다.<br>
<br>
즉, norm은 원점 o 에서 점 (x1, x2, ... ,xn) 까지 이르는 거리<br>
<br>

norm ||x|| = 1 일 경우<br>
2차원 -> 반지름이 1인 원<br>
3차원 -> 반지름이 1인 구<br>
4차원 -> 반지름이 1인 hyper 구<br>

<br>
<br>

### 2) 내적
<br>

-> dot product<br>
차원이 다른 두 벡터의 내적은 불가하다.<br>
x=(2,3) y=(1,4,2) -> x는 2차원, y는 3차원이므로 내적이 불가하다.<br>

### 3) 외적

-> cross product

<br>
<br>
<br>


## 3. 행렬 (matrix)
실수를 직사각형 모양으로 배열한 것

<br>

### 1) 행렬의 연산

#### 1)) 행렬의 덧셈과 뺄
-> 같은 차원을 가진 행렬끼리만 더하거나 뺄 수 있다.
<br>

#### 2)) 행렬의 곱셈
-> 행렬끼리 곱할 때는 차원을 주의해야 한다.
<br>

ex)<br>
axb인 경우<br>
a는 (3,2), b는 (2,3)의 차원을 가질 경우<br>
a의 뒤차원과 b의 앞차원이 같으므로 행렬 곱이 가능하며,<br>
a의 앞차원과 b의 뒤차원에 의해 연산 결과의 차원이 결정된다. -> (3,3)이 결과적으로 나온다. <br>
<br>
단, bxa는 연산이 불가하다.<br>

<br>
<br>

### 2) 전치행렬 (Transpose)
전치행렬은 원행렬의 행과 열을 뒤바꾼 행렬이다.

<pre>
A = 1 2 -1
    4 3 0

A transpose = 1 4
              2 3
             -1 0

4를 예시로 
A에서 4는 (2,1) 이다
A transpose는 (1,2)이다.
</pre>


<br>
<br>
<br>
<br>

# Numpy 소개 및 실습
<br>
<br>

## 1. 행렬 만들기
<br>

```python
# 행렬 만들기
import numpy as np
A = np.array([[1,2],
               [3,4]])
print(A)
'''
[[1 2]
 [3 4]]
 
-> 안쪽 []은 행을 표현하고, 각각의 행은 열을 갖는다.
'''
```

<br>
<br>
<br>

## 2. 행렬의 연산
<br>

### 1) 기본적인 행렬 연산
<br>

```python
# 행렬의 연산
A = np.array([[1,2],
               [3,4]])
print(A*3)
'''
[[ 3  6]
 [ 9 12]]
'''
print(A+A)
'''
[[2 4]
 [6 8]]
'''
print(A-A)
'''
[[0 0]
 [0 0]]
'''

```

<br>

### 2) element-wise operation
<br>

```python
A = np.array([[1,2],
               [3,4]])
print(A**2)
'''
-> A*A의 연산

[[ 1  4]
 [ 9 16]]
'''

A = np.array([[1,2],
               [3,4]])
print(A*A)
'''
-> A^2과 같은 결과가 나옴을 볼 수 있다.
(element wise 곱)
[[ 1  4]
 [ 9 16]]
'''

A = np.array([[1,2],
               [3,4]])
print(3**A)
'''
3^A -> 선형대수학에서는 존재하지 않음
각각의 A 값에 대해서 3에 제곱승을 한다.
[[ 3  9]
 [27 81]]
'''
```

<br>

### 3) 행렬 곱셈
<br>

- 행렬의 내적

```python
x = np.array([[1,2],[3,4]])
y = np.array([[3,4],[3,2]])

print(np.dot(x,y))
'''
행렬의 내적
[[ 9  8]
 [21 20]]
'''
```

<br>

- 행렬의 element-wise 곱

```python
x = np.array([[1,2],[3,4]])
y = np.array([[3,4],[3,2]])

print(x*y)
'''
행렬의 element-wise 곱
[[3 8]
 [9 8]]
'''
```

<br>

### 4) numpy array 비교연산
<br>

```python
a = np.array([1,2,3,4])
b = np.array([4,2,2,4])

# 비교연산을 통해 array 내의 값을 빠르게 비교 가능하다.

print(a == b) # [False  True False  True]
print(a > b) # [False False  True False]
```


<br>

### 5) numpy array 논리연산
<br>

```python
a = np.array([1,1,0,0], dtype=bool)
print(a) # [ True  True False False]
b = np.array([1,0,1,0], dtype=bool)
print(b) # [ True False  True False]

print(np.logical_or(a,b))
# [ True  True  True False]
print(np.logical_and(a,b))
# [ True False False False]
print(np.logical_xor(a,b))
# [False  True  True False]
print(np.logical_not(a))
# [False False  True  True]
```

<br>

### 6) numpy array Reductions
-> reduction: 어떤 numpy array가 있을 때 하나의 스칼라 값으로 만들어주는 연산의 모임 <br>

<br>

```python
a = np.array([1,2,3,4,5])

print(np.sum(a))
# 15
print(a.sum())
# 15

print(a.min()) # array에서 가장 작은 값
# 1
print(a.max()) # array에서 가장 큰 값
# 5
print(a.argmin()) # array에서 가장 작은 값의 위치(인덱스)
# 0
print(a.argmax()) # array에서 가장 큰 값의 위치(인덱스)
# 4
```


<br>

### 7) numpy array Logical Reductions
-> numpy array가 모두 boolean 으로 이루어져 있을 때 하나의 boolean 값으로 연산 결과를 냄 <br>
<br>

- all: Array 내의 모든 값이 True 인가?
- any: Array 내의 값이 하나라도 True 인가?

<br>

```python
a = np.array([True, True, True])
b = np.array([True, True, False])

print(np.all(a))
# True
print(np.all(b))
# False
print(np.any(a))
# True
print(np.any(b))
# True
```

<br>

### 8) numpy array Statical Reductions
<br>

```python
x = np.array([1,2,3,1])

print(np.mean(x)) # 평균값
# 1.75
print(np.median(x))
# 중간값 -> array를 오름차순으로 나열했을 때의 중간값 여기서는 1,1,2,3이므로 원소가 짝수개 이므로 (1+2)/2
# 1.5
print(np.std(x)) # 표준편차 -> array 안의 수가 얼마나 분산돼 있는가
# 0.82915619758885
```

<br>
<br>
<br>

## 3. Numpy 예제 문제

<br>
<br>

### 1) Numpy 행렬 만들기

```python
import numpy as np

def main():
    print(matrix_tutorial())

def matrix_tutorial():
    # Create the matrix A here...
    A = np.array([[1,4,5,8],
                  [2,1,7,3],
                  [5,4,5,9]])
    return A

if __name__ == "__main__":
    main()
```

<br>
<br>

### 2) Numpy 행렬 분산 구하기

```python
import numpy as np

def main():
    print(matrix_tutorial())

def matrix_tutorial():
    A = np.array([[1, 4, 5, 8], [2, 1, 7, 3], [5, 4, 5, 9]])
    A = A / np.sum(A)
    # 아래 코드를 작성하세요.

    return np.var(A)

if __name__ == "__main__":
    main()
```

<br>
<br>

### 3) 전치행렬 구하기

- 예외처리와 여러 값 입력에 주의하자.

```python
import numpy as np

def main():
    A = get_matrix()
    print(matrix_tutorial(A))

def get_matrix():
    mat = []
    [n, m] = [int(x) for x in input().strip().split(" ")] # 행과 열 입력 받기 (split을 통해 두 값을 구분)
    # input은 엔터치면 끝난다.
    for i in range(n):
        row = [int(x) for x in input().strip().split(" ")]
        mat.append(row)

    return np.array(mat)

def matrix_tutorial(A):

    B = np.transpose(A) # 전치행렬 # A.T 로 전치행렬 만들 수 있다.
    
    # 예외처리 숙지!!
    try:
        C = np.linalg.inv(B)# 역행렬

    except np.linalg.LinAlgError:
        return "not invertible"
    return np.sum(C > 0)


if __name__ == "__main__":
    main()
```



### 4) 벡터 연산과 numpy로 그림 그리기

- norm 을 이용하여 그림을 그린다.

<br>
<br>

```python
import matplotlib.pyplot as plt
import numpy as np

def circle(P):
    return np.linalg.norm(P) -1
# 위의 그림을 그리는 방식을 생각하면, 정확히 원 위에 있는 점들에 대해서 circle(P) 은 0을 가져야 합니다.
# x^2 + y^2 -1 = 0을 만족시키는 그래프

def diamond(P):
    return np.abs(P[0]) + np.abs(P[1]) -1
# |x| + |y| - 1 =0을 만족시키는 그래프


def smile(P):
    def left_eye(P):
        eye_pos = P - np.array([-0.5, 0.5])
        return np.sqrt(np.sum(eye_pos * eye_pos)) - 0.1

    def right_eye(P):
        eye_pos = P - np.array([0.5, 0.5])
        return np.sqrt(np.sum(eye_pos * eye_pos)) - 0.1

    def mouth(P):
        if P[1] < 0:
            return np.sqrt(np.sum(P * P)) - 0.7
        else:
            return 1

def checker(P, shape, tolerence):
    return abs(shape(P)) < tolerence

def sample(num_points, xrange, yrange, shape, tolerence):
    accepted_points = []
    rejected_points = []

    for i in range(num_points):
        x = np.random.random() * (xrange[1] - xrange[0]) - xrange[0]
        # (xrange[1] - xrange[0])= 기울기, np.random.random()= x, xrange[0]= b  -> 이를 통해 위치선정
        # np.random.random() -> 0 ~ 1사이의 수를 반환
        y = np.random.random() * (yrange[1] - yrange[0]) - xrange[0]

        P = np.array([x, y])

        if(checker(P, shape, tolerence)):
            accepted_points.append(P)
        else:
            rejected_points.append(P)
    return np.array(accepted_points), np.array(rejected_points)

xrange = [-1.5, 1.5]
yrange = [-1.5, 1.5]

accepted_points, rejected_points = sample(100000, xrange, yrange, diamond, 0.005)
plt.figure(figsize=(xrange[1]-xrange[0], yrange[1]-yrange[0]), dpi=150)

plt.scatter(rejected_points[:,0], rejected_points[:,1], c='lightgray', s=0.005) # s= 점의 크기
plt.scatter(accepted_points[:,0], accepted_points[:,1], c='black', s=1)

plt.show()
```

<br>
<br>
<br>
<br>

# Regression analysis
<br>

## 1. 단순선형회귀분석
-> 데이터를 가장 잘 설명하는 어떤 선을 하나 찾는다. <br>

**cf) 용어 정리**
<br>

N: 데이터의 개수<br>
X: input 데이터<br>
Y: output 데이터<br>
xi, yi: i번째 데이터<br>
<br>
모델: 입력에대한 예측값을 출력<br>

<br>

### 1) 단순선형회귀분석의 목표와 가정
<br>

가정: 데이터가 선형적 관계를 이룬다.<br>
y = wx+b<br>
<br>
-> 데이터에 가장 적합한 기울기 w와 절편 b를 구하는 것이 단순선형회귀분석의 목표이다.<br>

<br>

### 2)  단순선형회귀분석 코드

```python
import matplotlib.pyplot as plt
import numpy as np


# 실습에 필요한 데이터입니다. 수정하지마세요.
X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]

w = 0.5
b = 2
x= [0,10]

fig, ax = plt.subplots()
ax.scatter(X, Y)
ax.plot(x, [b , w * 10 + b], c='r') # (0, w * 10 + b) 와 (10, w * 10 + b)의 점을 연결하는 선
plt.xlim(0,10)
plt.ylim(0,10)
plt.show()
```

<br>

- 단순선형회귀분석 그래프

![image](/assets/rgg.png)

<br>
<br>

### 3)  모델의 학습 목표

각 데이터 (xi, yi)의 실제 값과 모델이 예측하는 값의 차를 최소한으로 하자!

- 차이: yi - (w * xi + b)
- 전체 모델의 차이: sum((yi - (w * xi + b))^2) 
    - 제곱을 하는 이유는 양수와 음수의 합에 의해 전체 모델의 차이가 감소되는 것을 막기 위해
    - 위 값을 loss function이라고 한다.
    - loss function을 최소로하는 w,b를 구한다. (argmin(sum((yi - (w * xi + b))^2) ))

<br>
<br>


### 4) loss function (MSE)

![image](/assets/mse.png)


### 5) loss function을 이용하여 loss 구하기 코드

<br>

```python
import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np

def loss(x, y, beta_0, beta_1):
    N = len(x)

    loss_value = [loss_function(x[n], y[n],beta_0,beta_1) for n in range(N)]

    return np.sum(loss_value)

def loss_function(x, y, w, b):
    return (y - (w * x +b))**2

X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513,
     5.48321616]
Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441,
     5.19692852]

beta_0 = 1  # 기울기
beta_1 = 0.5  # 절편
print(loss(X, Y, beta_0, beta_1))

fig, ax = plt.subplots()
ax.scatter(X,Y)
ax.plot([0,10],[beta_1, beta_0 * 10 + beta_1],c='red')
plt.show()

```

<br>
<br>
<br>

## 2. gradient descent
### 1) loss를 최소로하는 w,b를 어떻게 찾을것인가?

<br>

<pre>
아무 곳에서나 시작했을 때 가장 정상을 빠르게 찾아가는 방법은 무엇일까?

가정: 1. 정상의 위치는 알 수 없다
      2. 현재 나의 위치와 높이를 알 수 있다.
      3. 재 위치에서 일정 수준 이둉할 수 있다.
      
방법: 1. 현재 위치에서 가장 경사가 높은 쪽을 찾는다.
      2. 오르막 방향으로 일정수준 이동한다.
      3. 더 이상 높이의 변화가 없을 때까지 반복한다.
</pre>

<br>

거꾸로 된 산을 내려가기<br>
<br>
데이터를 가장 잘 설명하는 w,b를 구하자<br>
= 예측값과 실제값의 차이를 최소로 만드는 값을 구하자<br>
= loss function을 최소로 만드는 w,b를 구한다.<br>
<br>

<br>

### 2) gradeint descent로 오차 줄이기 코드

```python
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression


def loss(x, y, beta_0, beta_1):
    N = len(x)

    loss_value = [loss_function(x[n], y[n],beta_0,beta_1) for n in range(N)]

    return np.sum(loss_value)

def loss_function(x, y, w, b):
    return (y - (w * x +b))**2


X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513,
     5.48321616]
Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441,
     5.19692852]

train_x = np.array(X).reshape(-1,1)
# 하나의 속성(feature)에 여러가지 값(sample)을 가지는 경우, reshape(-1, 1)을 적용하여 열벡터로 만들어야 합니다.
train_y = np.array(Y).reshape(-1,1)

lrmodel = LinearRegression()
lrmodel.fit(train_x, train_y)

beta_0 = lrmodel.coef_[0] # -> lrmodel로 구한 직선의 기울기
beta_1 = lrmodel.intercept_ # -> lrmodel로 구한 직선의 bias

print(loss(X, Y, beta_0, beta_1))

fig, ax = plt.subplots()
ax.scatter(X, Y)

ax.plot([0,10], [beta_1, beta_0 * 10 + beta_1], c='r')

plt.show()
```

## 3. 다중회귀분석
<br>


### 1) 다중회귀분석
X: vector(x^1, x^2, x^3)<br>
Y: scalar(y)<br>
-> 다중회귀분석은 입력으로 벡터를 주면 하나의 스칼라값을 결과로 한다.<br>
<br>
가정: 데이터 y와 x^1, x^2, x^3는 선형적 관계를 이루고 있다.<br>
      y = w * x^1 + w * x^2 + w* x^3 + b<br>

푸는 방법은 단순회귀분석과 동일하다.<br>
loss function은 w * x^1 + w * x^2 + w* x^3 + b로 확장시키면 된다.<br>

<br>

## 2) 다중회귀분석 코드

- 변수가 많아졌을 뿐이다.

```python
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import csv

def expected_sale(x, w, b):
    result = np.dot(x, w) + b
    return np.array(result).astype(np.float32)


csvreader = csv.reader(open("Advertising.csv"))
x = []
y = []

next(csvreader) # label 부분을 제외 ['', 'FB', 'TV', 'Newspaper', 'Sales']

for data in csvreader:
    x.append(data[1:4])
    y.append(data[-1])

train_x = np.array(x).astype(np.float64)
train_y = np.array(y)


LR = LinearRegression()
LR.fit(train_x, train_y)

w = np.array(LR.coef_).astype(np.float64).reshape(-1,1)

b = np.array([LR.intercept_]).astype(np.float64)

cost = np.array([10, 12, 3]).astype(np.float64).reshape(1,-1)

print(expected_sale(cost, w, b)[0][0])
```























































