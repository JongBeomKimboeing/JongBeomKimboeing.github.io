---
layout: post
title: MNIST with Neural Network
description: "MNIST with Neural Network"
modified: 2020-05-08
tags: [김성훈,DL]
categories: [김성훈DL]
---
```python
import tensorflow as tf
import numpy as np
import random as rand
import matplotlib.pyplot as plt
import time

rand.seed(777)
learning_rate = 0.01
batch_size = 100
training_epoch = 15
nb_classes = 10

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train/255.0, x_test/255.0

x_train = tf.reshape(x_train, shape=[x_train.shape[0], x_train.shape[1]*x_train.shape[2]])
x_test = tf.reshape(x_test, shape=[x_test.shape[0], x_test.shape[1]*x_test.shape[2]])

y_train = tf.keras.utils.to_categorical(y_train, nb_classes)
y_test = tf.keras.utils.to_categorical(y_test, nb_classes)

start = time.time()

tf.model = tf.keras.Sequential()
tf.model.add(tf.keras.layers.Dense(units=2000, input_dim = x_train.shape[1] , activation='relu'))
tf.model.add(tf.keras.layers.Dense(units=2000, activation='relu'))
tf.model.add(tf.keras.layers.Dense(units=2000, activation='relu'))
tf.model.add(tf.keras.layers.Dense(units=2000, activation='relu'))
tf.model.add(tf.keras.layers.Dense(units=nb_classes, activation='softmax'))
tf.model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=['accuracy'])
tf.model.summary()

history = tf.model.fit(x_train, y_train, batch_size= batch_size, epochs=training_epoch)

prediction = tf.model.predict(x_test)
print("prediction: \n",prediction)
accuracy = tf.model.evaluate(x_test, y_test)
print("accuracy: ",accuracy[1])

r = rand.randint(0, x_test.shape[0]-1)
print("Label:", np.argmax(y_test[r]))
print("predicted:", np.argmax(prediction[r]))
print("time: %d sec" %(time.time() - start))
plt.imshow(x_test[r].numpy().reshape(28,28), cmap='Greys', interpolation='nearest')
plt.show()
```
training set의 결과
```python
loss: 0.1555 - accuracy: 0.9603
```
test set의 결과
```python
accuracy:  0.9468
Label: 8
predicted: 1
time: 155 sec
```
# 고찰
단순 nn으로 처리한 mnist는 생각보다 정확도가 떨어진다.<br>
training set과 test set의 결과를 비교해보면,<br>
training set의 accuracy가 test set의 accurcy보다 2% 높다는 걸 확인할 수 있다<br>
이러한 결과를 보아 설계한 신경망이 overfitting(과대적합) 됐음을 알 수 있다.
