---
layout: post
title: Machine Learning Basis 2
description: "Machine Learning Basis"
modified: 2020-06-28
tags: [Machine Learning]
categories: [Machine Learning]
---

# 확률의 기초
<br>

## 1. 확률의 정의
어떤 사건이 일어날 것인지 혹은 일어났는지에 대한 지식 혹은 믿음을 표현하는 방법

<br>
<br>

## 2. 확률의 연산
-> statics 참고

<br>
<br>

## 3. 조건부 확률
<br>

### 1) 조건부 확률의 정의
<br>
사건 B가 일어났을 때 A가 일어날 확률<br>
P(A|B) = P(A  교집합 B) / P(B)<br>

<br>

ex)<br>

<pre>
ex) 사건 A: 주사위에서 짝수가 나오는 사건 (A:{2.4.6})
    사건 B: 주사위에서 3보다 큰 수가 나오는 사건 (B:{4,5,6})
    P(A|B) = 3보다 큰 수가 나왔을 때 그 수가 짝수일 확률
           = P({4,6}) / P({4,5,6}) = 2/3

    P(B|A) = 짝수가 나왔을 때 그 수가 3보다 클 확률
           = P({4,6}) / P({2,4,6}) = 2/3
</pre>

<br>
<br>

## 4. 몬테카를로 방법
<br>

### 1) 몬테카를로 방법 정의

물리시간에 배웠던 빨대 던지기이다.<br>
어떤 도형이 있고, 임의로 빨대를 던져 도형안에 들어간 빨대의 개수를 셈으로서 도형의 넓이를 구할 수 있다.<br>
-> 확률을 이용하여 어떤 도형의 넓이를 구하는 방식이다.<br>

<br>
<br>

### 2) 몬테카를로 방법 코드

```python
import matplotlib.pyplot as plt
import numpy as np

def main():
    plt.figure(figsize=(5,5))

    x = []
    y = []

    N = 10000

    for i in range(N):
        x.append(np.random.rand() * 2 -1) # [0 ~ 1] 값을 -> [0 ~ 2] 값으로 -> [-1 ~ 1] 값으로 변경
        y.append(np.random.rand() * 2 -1) # [0 ~ 1] 값을 -> [0 ~ 2] 값으로 -> [-1 ~ 1] 값으로 변경
    x = np.array(x)
    y = np.array(y)
    distance_from_zero = np.sqrt(x*x+y*y) # norm 구하기
    #print(distance_from_zero)
    is_inside_circle = distance_from_zero <= 1 # norm이 1이하인 값이면 원 안에 존재
    #print(is_inside_circle)

    print("Estimated pi = %f" % (np.average(is_inside_circle) * 4))

    plt.scatter(x,y,c=is_inside_circle)
    plt.show()



if __name__ == "__main__":
    main()
```

<br>
<br>


## 5. 베이즈 법칙

빈도주의자 vs 베이즈주의자<br>
(frequentist)  (Bayesian)<br>
"동전 하나를 던졌을 때 앞면이 나올 확률은 50%이다."<br>
<br>

### 1) 빈도주의자

<br>
이 동전을 수천, 수만 번 던졌을 때<br>
그 중 앞면이 50%, 뒷면이 50%가 나온다.<br>
(사건을 계속 관찰했을 때 사건의 수가 무한해지면, 사건의 확률을 정확히 정할 수 있다.)<br>
<br>

### 2) 베이즈주의자
<br>

동전 던지기의 결과가<br>
앞면이 나올 것이라는 확신(혹은 믿음)이 50%이다.<br>
(사전 지식을 바탕으로 사건에 대한 믿음 혹은 확신으로 확률을 정의한다.)<br>
<br>
<br>
베이즈주의와 빈도주의가 일맥상 통하는 이유는<br>
베이즈주의가 사전지식을 알고 있다고 하더라도, 사전 지식은 사건을 관찰함으로써 점점 업데이트가 된다.<br>
-> 결국 확률이 어떤 곳으로 수렴하는 지 관찰하는 것은 같다.<br>

<br>
<br>


## 6. 베이즈 법칙의 유도




















































































































