---
layout: post
title: Machine Learning Basis 3
description: "Machine Learning Basis"
modified: 2020-06-28
tags: [Machine Learning]
categories: [Machine Learning]
---

# 비지도학습 개론

## 1. 비지도 학습
<br>

비지도학습<br>
: 라벨링이 돼 있지 않은 데이터를 학습시켜 답이 정해져 있지 않은 데이터에서 숨겨진 구조를 파악한다.<br>
-> 데이터가 가지고 있는 구조에 의해 비슷한 구조를 가지고 있는 데이터끼리 뭉친다.<br>
<br>
<br>
<br>


# hard vs soft clustering

## 1. hard clustering
<br>

hard clustering: 데이터 포인트들은 비슷한 것들끼리 뭉쳐있다.<br>
-> 섞일 수 없는 데이터를 구분<br>
ex) 고양이와 강이지 구분<br>
    구분 선을 기준으로, 100% 고양이인 영역과 100% 강아지인 영역이 나뉜다 <br>
<br>
<br>

## 2. soft clustering
<br>

soft clustering: 한 개의 데이터 포인트는 숨겨진 클러스터들의 결합니다.<br>
-> 섞일 수 있는 데이터를 구분<br>
<br>

ex) 
책 장르 (과학, 역사, 판타지 기준으로)<br>
<br>
이 책은 과학 60%, 역사 5%, 판타지 35% 의 비율로 섞여있다<br>
-> 사이언스 픽션<br>
<br>
이 책은 과학 0% 역사 45%, 판타지 55% 의 비율로 섞여있다<br>
-> 역사 판타지 <br>
<br>
**즉, 어떤 구분선으로 나눌 수 없다**

<br>
<br>

## 3. hard clustering 과  soft clustering 의 대표적인 알고리즘

### hard clustering 의 대표적인 알고리즘
1) K-Means clustering<br>
2) Hierarchical Clusering<br>
3) DBSCAN<br>
4) OPTICS<br>
<br>

### soft clustering 의 대표적인 알고리즘<br>
1) Gaussian Mixture Models (EM)<br>
2) Soft K-Means<br>
3) Topic Models<br>
4) FCM<br>
<br>

cf) 자연적으로 일어난 데이터는 soft clustering이 많이 쓰임<br>
    그러므로, 일반적으로 soft clustering을 많이 쓴다.<br>
<br>
<br>

## 4. hard clustering 의 목표

hard clustering 의 목표<br>
-> 비슷한 데이터 포인터끼리 모은다.<br>
<br>
ex) 사람이 비슷하게 뭉친 클러스터 2개(k개)를 찾아달라고 명령을 한다.<br>
-> 컴퓨터는 각각의 데이터포인트 사이의 거리를 가지고 비슷한 것 끼리 뭉치게한다.<br>
<br>
<br>

## 5. k 선택 시 고려할 것들

1) 데이터의 특성<br>
<br>

- 어떻게 만들어진 데이터인가? (데이터의 소스가 어떤 특성을 가지고 있는가?)<br>

- 데이터 포인트 외 다른 feature<br>
<br>

ex) 사람의 몸무게<br>
- 만약 데이터가 남성과 여성의 데이터로 나뉘어 져 있다. <br>
   (일반적으로 남성이 여성보다 몸무게가 많이 나가기 때문에 k=2로 설정)<br>
   
- 남성 중에서 운동선수인 사람과 평범한 사람<br>
   (일반 남성과 운동선수인 사람으로 나뉘므로 k=2)<br>
<br>
<br>

2) 분석 결과로 얻고자 하는 것<br>
<br>

- 고양이 vs 개 분류  (k=2)
- 사람들의 행동 분석
<br>
ex) 사람의 행동을 5개로 분류할 수 있다.(k=5)<br>

- 가격 대비 효율성 분석<br>

<br>
<br>
<br>
<br>

# PCA 차원 축소법

## 1. 차원축소
데이터의 특성이나 데이터에대한 사전지식이 없을 때, k를 결정하는 것을 도와준다.
<br>
<br>

ex)<br>
내가 와이너리의 데이터 사이언티스트이다.<br>
지금까지 와이너리 회사에서 수백 가지의 와인을 생산했고,<br>
생산팀은 그 와인들에 대한 13가지 특성을 측정해서 정리했다.<br>
<br>
**문제**
지금까지 생산한 와인들을 종류별로 모아서 라인업으로 만들고 싶다.<br>
178개의 와인들을 몇 가지로 분류할 수 있을까?<br>
<br>

-> k를 결정하는 데 있어서 가장 쉽게 결정하는 방법은 눈으로 데이터를 봐서 k를 결정하는 것이다.<br>
그러나, 현재 데이터는 13가지 특성으로 나뉘어 있다.<br>
13가지 특성으로 나뉘어 있는 현재 데이터를 시각화 하면, 13차원의 그래프로 나타내어진다.<br>
13차원은 우리가 이해할 수 없다.<br>
<br>
그러므로, 우리는 최소한 이 그래프를 2차원 또는 3차원으로 줄여야 눈으로 데이터를 확인하고 이해 가능하다.<br>
-> 이렇게 차원을 줄이기 위해 사용하는 것이 PCA이다.<br>
<br>
<br>

## 2. 차원축소 목표
어떤 고차원의 데이터를 2차원/3차원으로 줄여서 데이터들을 2차원/3차원 형태로 보는 것이 목표이다.<br>
-> 이를 통해 클러스팅을 할 때 k를 몇으로 할 지 정해줄 수 있다.<br>
<br>
<br>

## 3. 주성분 분석 (PCA)

***PCA (Principal Component Analysis) (주성분 분석)***

### 1) PCA 를 왜 사용하는가?
1) 고차원 데이터를 저차원으로 줄이기 위해 (시각화 목적)

2) 고차원에서 저차원으로 줄일 때 발생하는 데이터 손실을 최소화하는 것이 목표이다.

3) 데이터 정제를 위해 사용
-> 3차원 데이터인데, 데이터들이 2차원 평면을 이루고 있는 경우 3차원 데이터들을 2차원으로 투영시킨다.<br>

   3차원 데이터가 2차원 평면을 이루고 있는 경우는 주로, 데이터를 수집할 때 발생한 noise에 의한 것이다.<br>
   만약, 이 데이터가 사실은 2차원 데이터에서 나온 것이 확실하다면 이것을 3차원에서 2차원으로 줄임으로써<br>
   쓸때 없는 noise를 제거할 수 있다.<br>
   
<br>
**PCA 는 주로 데이터 정제에 많이 사용된다.**
왜냐하면 k-means 나 여러 clustering 방법은 단지 데이터간의 거리를 측정하기만 하면 clustering 을 할 수 있어서<br>
데이터의 차원에 영향을 받지 않는다.<br>
<br>
차원의 영향을 받지 않음에도 불구하고 PCA를 사용하는 이유는<br>
데이터의 noise 를 줄여 데이터 정제를 하기 위해서이다.<br>
<br>
차원이 높다는 것은 데이터를 표현할 수 있는 자유도가 높다는 의미이다.<br>
자유도가 너무 높을 경우 모듈이 제대로 작동하지 않는다.<br>
그렇기 때문에 적당한 차원으로 줄여 자유도를 제한할 필요가 있다.<br>
<br>
<br>

### 2) PCA 차원축소 코드

```python
import csv
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits.mplot3d import Axes3D

def main():
    X, attributes = input_data()
    pca_array = normalize(X)
    pca, pca_array = run_PCA(X, 3)
    visualize_3d_wine(pca_array)
    pca, pca_array = run_PCA(X, 2)
    visualize_2d_wine(pca_array)

def input_data():
    x = []
    attributes = []
    with open('wine.csv') as wine:
        x = list(csv.reader(wine, delimiter=','))

    with open('Wine_attributes.txt') as at:
        attributes = [line.replace('\n','') for line in at.readlines()]
    return np.array(x).astype(float), attributes

def normalize(x):
    # 하나의 feature마다 scailing을 한다.
    for i in range(x.shape[1]): # x.shape: (178,13)
        # (3,4,5)가 있고 이를 가장 작은 값을 0, 가장 큰 값을 1로 만들어주고 싶다면
        # (3,4,5) 에서 가장 작은 값을 빼고 -> (0,1,2)
        # (0,1,2) 에서 가장 큰 값으로 나누면 -> (0, 1/2, 1) 이 된다.
        x[:,i] = x[:,i] - np.min(x[:,i])
        x[:, i] = x[:,i] / np.max(x[:,i])
    return x


def run_PCA(X, num_components):
    pca = PCA(n_components=num_components)
    pca.fit(X)
    pca_array = pca.transform(X)
    print(pca_array)

    return pca, pca_array


def visualize_3d_wine(X):
    # X를 시각화하는 코드를 구현합니다.
    fig = plt.figure()
    ax = fig.gca(projection='3d')
    ax.scatter(X[:,0], X[:,1],X[:,2])

    plt.savefig("image3d.png")
    plt.show()

def visualize_2d_wine(X):
    fig, ax = plt.subplots()
    plt.scatter(X[:,0], X[:,1])
    plt.savefig("image2d.png")


if __name__ == '__main__':
    main()
```

<br>
<br>
<br>
<br>

# k-means 이론

**k-means : 반복을 이용한 클러스터링 알고리즘**
-> 한 번의 반복에 대해 이해하면 그것을 계속 사용 가능하다.<br>

## 1) 중심과 중심과의 거리

1) 중심 (centroid): 각 클러스터의 "중심"을 의미<br>
 중심 정하는 방법: 각각의 클러스터 안에 있는 데이터 포인터들의 x좌표의 평균값과 y좌표의 평균값으로 중심을 구한다.<br>
<br>

2) 중심과의 거리 (distance): 중심과 데이터 포인트와의 거리<br>
   -> 어떤 데이터 포인트가 있을 때, 이 데이터 포인트가 중심과 얼마의 거리를 가지고 있다. (거리는 norm으로 결정한다.)<br>
<br>
<br>

## 2) k-means 알고리즘
-> k-means 는 중심(centroid)의 위치에 의해 클러스터링을 진행한다.<br>
<br>

1) k-means step0
   알고리즘을 시작할 때, 초기 중심값은 데이터 중 임의로 선정한다.
<br>

2) k-means step1
   중심값이 정해지면, 각각의 데이터 포인트에 대해 다음을 계산:
   "내게서 가장 가까운 중심점은 어딘가?" (norm으로 계산)
   -> 이후, 데이터 포인트에 가장 가까운 중심에 해당하는 클러스터에 저장한다.
      ex) 주황색 점에서 가장 가까운 중심점이 중심B라면, 이 데이터 포인트를 클러스터 B에 할당한다.
<br>

3) k-means step1을 모든 데이터에 대해 동일한 작업을 수행한다.
<br>

4) k-means step2
   정해진 클러스터에서 중심점을 다시 계산한다.
   중심점은 해당 클러스터 내 데이터 포인트 위치의 무게중심값 또는 평균으로 계산한다.
   (중심의 평균이나 무게중심을 찾을 때, 각 데이터 포인트의 x좌표와 y좌표의 평균/중심을 이용한다.)
   <br>
   
5) k-means step1
   다시 k-means step1을 반복한다.<br>
   <br>
    중심값이 정해지면, 각각의 데이터 포인트에 대해 다음을 계산:<br>
   "내게서 가장 가까운 중심점은 어딘가?" (norm으로 계산)<br>
   -> 이후, 데이터 포인트에 가장 가까운 중심에 해당하는 클러스터에 저장한다.<br>
      ex) 주황색 점에서 가장 가까운 중심점이 중심B라면, 이 데이터 포인트를 클러스터 B에 할당한다.<br>
      <br>
      
6) k-means step2
   다시 k-means step2를 반복한다.<br>
    정해진 클러스터에서 중심점을 다시 계산한다.<br>
   중심점은 해당 클러스터 내 데이터 포인트 위치의 무게중심값 또는 평균으로 계산한다.<br>
   (중심의 평균이나 무게중심을 찾을 때, 각 데이터 포인트의 x좌표와 y좌표의 평균/중심을 이용한다.)<br>
   <br>
   
7) k-means step1과 k-means step2 를 반복하는데,<br>
   중심을 엄데이트 했을 때, 어떠한 데이터 포인트에 새로운 클러스터 할당이 일어나지 않았을 때 알고리즘이 종료된다.<br>

<br>
<br>

## 3) k-means clustering 코드

```python
import sklearn.decomposition
import sklearn.cluster
import matplotlib.pyplot as plt
import numpy as np


def main():
    X, attributes = input_data()
    X = normalize(X)
    pca, pca_array = run_PCA(X, 2)
    labels = kmeans(pca_array, 3, [0, 1, 2]) # 시작점에 따라 clustering이 다르게 될 수 있다.
    visualize_2d_wine(pca_array, labels)


def input_data():
    X = []
    attributes = []

    with open('wine.csv') as fp:
        for line in fp:
            X.append([float(x) for x in line.strip().split(',')])

    with open('Wine_attributes.txt') as fp:
        attributes = [x.strip() for x in fp.readlines()]

    return np.array(X), attributes


def run_PCA(X, num_components):
    pca = sklearn.decomposition.PCA(n_components=num_components)
    pca.fit(X)
    pca_array = pca.transform(X)

    return pca, pca_array


def kmeans(X, num_clusters, initial_centroid_indices):
    import time
    N = len(X)
    centroids = X[initial_centroid_indices]
    # [-7.06335756e-01 - 2.53192753e-01] -> 첫 번째 cluster 중심
    # [-4.84976802e-01 - 8.82289142e-03] -> 두 번째 cluster 중심
    # [-5.21172266e-01 - 1.89187222e-01] -> 세 번째 cluster 중심
    labels = np.zeros(N)  # 모든 데이터가 0번째 클러스터에 들어가있다고 초기 설정을 해둔다

    while True:
        #Step 1. 각 데이터 포인트 i 에 대해 가장 가까운
        #중심점을 찾고, 그 중심점에 해당하는 클러스터를 할당하여
        #labels[i]에 넣습니다.
        #가까운 중심점을 찾을 때는, 유클리드 거리를 사용합니다.
        #미리 정의된 distance 함수를 사용합니다.
        is_changed = False
        for i in range(N):
            distances = []
            for k in range(num_clusters):
                # x[i]와 centroid[k] 사이의 거리를 구하고, 거리가 가장 작은 k번째 index를 label에 넣는다.
                k_dist = distance(X[i], centroids[k]) # 하나의 데이터포인트에 대한 각각의 중심점까지의 거리
                distances.append(k_dist)
            if labels[i] != np.argmin(distances): # 이전 label에 들어간 값이 현재 label에 들어갈 값과 다른 경우 -> True
                is_changed=True
            labels[i] = np.argmin(distances) # 가장 가까운 위치의 distance에 해당하는 index를 labels에 저장



        #Step 2. 할당된 클러스터를 기반으로 새로운 중심점을 계산합니다.
        #중심점은 클러스터 내 데이터 포인트들의 위치의 *산술 평균*
        #으로 합니다.

        for k in range(num_clusters):
            x = X[labels == k][:,0] # X가 데이터의 x,y좌표를 가지고 있으므로, [:,0]->x좌표 [:,1]->y좌표
            y = X[labels == k][:,1]

            x = np.mean(x)
            y = np.mean(y)
            centroids[k] = [x, y]
        print(centroids)


        #Step 3. 만약 클러스터의 할당이 바뀌지 않았다면 알고리즘을 끝냅니다.
        #아니라면 다시 반복합니다.
        
        if not is_changed:
            break
    return labels


def distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2) ** 2))


def normalize(X):
    for dim in range(len(X[0])):
        X[:, dim] -= np.min(X[:, dim])
        X[:, dim] /= np.max(X[:, dim])
    return X



#이전에 더해, 각각의 데이터 포인트에 색을 입히는 과정도 진행합니다.



def visualize_2d_wine(X, labels):
    plt.figure(figsize=(10, 6))
    plt.scatter(X[:,0], X[:,1], c=labels)
    plt.savefig("image.svg", format="svg")


if __name__ == '__main__':
    main()

```

<br>
<br>

## 3) k-means clustering 을 이용한 이미지 처리

```python
from PIL import Image
import numpy as np



def main():
    """
    불러온 이미지를 RGB 값을 갖는 3차원 벡터로 변환합니다.
    """

    # 이미지를 불러옵니다.
    filenames = ["aurora.jpg", "./data/raccoon.jpg"]
    img = Image.open(filenames[0])
    img = img.convert("RGB")

    # 이미지를 NumPy 배열로 변환합니다. (높이, 넓이, 3) 차원의 행렬로 변환하여 돌려줍니다.
    image_vector = np.asarray(img)
    prep_image_vector = preprocess(image_vector)

    # K-means의 K값을 설정합니다.
    K = 32

    new_image, clusters, centroids = kmeans(prep_image_vector, K)
    new_image = postprocess(new_image)

    # 변환된 벡터의 타입을 처리된 이미지와 같이 8bits로 설정합니다.
    new_image = new_image.astype("uint8")
    # 데이터를 이미지로 다시 변환합니다.
    new_img = Image.fromarray(new_image, "RGB")
    # 이미지를 저장하고 실행결과를 출력합니다.
    new_img.save("image1out.jpg")

    # 점수를 확인합니다.


def kmeans(image_vector, K=32):
    """
    클러스터링을 시작합니다. pick_seed를 이용해 초기 Centroid를 구합니다.
    regress는 이용해 클러스터를 할당(allocate_cluster)하고 새로운 Centroid를 계산(recalc_centroids)합니다.
    """

    clusters = np.zeros((image_vector.shape[:2]))
    centroids = np.array(pick_seed(image_vector, K))

    while True:
        new_clusters, new_centroids = regress(image_vector, centroids)
        print(new_clusters)
        print(new_centroids)
        if np.all(clusters == new_clusters):
            break
        clusters, centroids = new_clusters, new_centroids
        break

    new_image = np.zeros(image_vector.shape)

    for i in range(K):
        new_image[clusters == i] = centroids[i]


    return new_image, clusters, centroids


def regress(image_vector, centroids):
    new_clusters = allocate_cluster(image_vector, centroids)
    new_centroids = recalc_centroids(image_vector, new_clusters, centroids.shape[0])

    return new_clusters, new_centroids


def pick_seed(image_vector, K):
    """
    image_vector로부터 K개의 점을 선택하여 리턴해주세요!
    """

    centroids = np.zeros((K, image_vector.shape[2]))
    for i in range(K):
        centroids = image_vector[i]
    return centroids


def allocate_cluster(image_vector, centroids):
    height, width, _ = image_vector.shape
    clusters = np.zeros((height, width))
    k_dist=[]
    """
    주어진 Centroid에 따라 새로운 클러스터에 할당해주세요.

    예를들어, 0행 0열의 pixel이 3번 Centroid에 가깝다면, clusters[0][0] = 3 이라고 채웁니다.

    """
    for i in range(height):
        for j in range(width):
            for k in centroids:
                k_dist = distance(clusters[i][j], k)


    k_dist = np.array(k_dist)
    k_dist = np.argmin(k_dist)

    clusters = k_dist


    return clusters


def recalc_centroids(image_vector, clusters, K):
    centroids = np.zeros((K, image_vector.shape[2]))
    height, width, _ = image_vector.shape
    """
    Cluster별로 새로운 centroid를 구해서 되돌려줍니다.

    """
    for i in range(height):
        for j in range(width):
            for k in range(K):
                x = clusters[clusters[i][j]== k][:,1] # X가 데이터의 x,y좌표를 가지고 있으므로, [:,0]->x좌표 [:,1]->y좌표
                y = clusters[clusters[i][j]== k][:,2]
                z = clusters[clusters[i][j]== k][:,3]

                x = np.mean(x)
                y = np.mean(y)
                z = np.mean(z)
                centroids[k] = [x, y, z]


    return centroids

def distance(x, y):
    return np.sqrt(np.sum((x - y) ** 2))

def preprocess(image_vector):
    """
    이미지 전처리 함수

    이 함수를 필히 작성하지 않아도 동작합니다.
    """
    return image_vector


def postprocess(image_vector):
    """
    이미지 후처리 함수

    이 함수를 필히 작성하지 않아도 동작합니다.
    """
    return image_vector


if __name__ == "__main__":
    main()
```





































































































