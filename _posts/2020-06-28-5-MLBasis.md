---
layout: post
title: Machine Learning Basis 5
description: "Machine Learning Basis"
modified: 2020-06-28
tags: [Machine Learning]
categories: [Machine Learning]
---

# tensorflow 기초 사용법
<br>

## 1. 상수형 텐서 선언 -> 절대 변하지 않는 값

```python
import tensorflow as tf

tensor_a = tf.constant(value, dtype=None, shape= None, name=None)
```

<br>
<br>

## 2. 모든 원소 값이 0인 tensor 생성

```python
import tensorflow as tf

tensor_b = tf.zeros(shape, dtype=tf.float32, name=None)
-> 데이터 초기화할 때 사용
-> shape에는 튜플을 넣어줘야함 ex) (1,2)=1행2열
```
<br>
<br>

## 3. 모든 원소 값이 1인 tensor 생성

```python
import tensorflow as tf

tensor_c = tf.ones(shape, dtype=tf.float32, name=None)
-> 데이터 초기화할 때 사용
-> shape에는 튜플을 넣어줘야함 ex) (1,2)=1행2열
```
<br>
<br>

## 4. 시퀀스 선언하기

```python
import tensorflow as tf

tesor_d = tf.linspace(start=, stop=, num=,name=)

'''
start: 시작 값
stop: 끝 값
num: 생성할 데이터 수
name: 시퀀스 이름
-> 시작 값 부터 끝 값까지 num의 개수만큼 생성
'''
```

<br>
<br>

## 5. start에서 stop까지 delta씩 증가하는 데이터

```python
import tensorflow as tf

tensor_e = tf.range(start=,limit=,delta=,name=)

'''
start: 시작값
limit: 끝 값
delta: 증가량
name: 시퀀스 이름
'''
```

<br>
<br>

## 6. 난수 선언하기
-> 난수를 통해 tensor를 초기화해주는 경우가 많고, 난수로 초기화 시켜주면 성능이 더 높게 나오기도 한다.<br>

### 1) 정규분포 생성

```python
# 정규분포 생성
tensor_f = tf.random.normal(shape=, mean=, stddev=, dtype=, seed=, name=)
->mean: 평균 (주로 0으로 지정)
->stddev: 표준편차(주로 1로 지정)
```

### 2) 균등분포 생성

```python
# 균등분포 생성
tf.random.uniform(shape=, minval=, maxval=, dtype=, seed=, name=)
-> -1과 1사이의 값을 균등한 확률로 random하게 값을 지정하고 싶은 경우
(정규분포는 정규분포 그래프를 참고하면, 0 근처의 값이 나올 확률이 높을 수 있다는 것을 볼 수 있다.)
```

<br>
<br>

## 7. 변수선언

```python
tensor_f = tf.Variable(value, name=)

# 일반적인 퍼셉트론의 가중치와 bias 생성
weight = tf.Variable(10)
bias = tf.Variable(tf.random.normal([10,10])) # 정규분포로 초기화
```

<br>
<br>

## 8. 연산자

```python
tf.negative(x) # -x (숫자만)
tf.logical_not(x) # !x (boolean 값만)
tf.abs(x) # x의 절대값 (숫자만)

tf.add(x,y) # x + y
tf.subtract(x,y) # x - y
tf.multiply(x,y) # x * y
tf.truediv(x,y) # x / y
tf.math.mod(x,y) # x % y
tf.math.pow(x,y) # x ** y
```

<br>
<br>

## 9. 사용 예시


### ex1) tensor 생성 

```python
def constant_tensors():
    # 5의 값을 가지는 (1,1) shape의 8-bit integer 텐서를 만드세요.
    t1 = tf.constant(value=5, shape=(1,1),dtype=tf.int8)

    # 모든 원소의 값이 0인 (3,5) shape의 16-bit integer 텐서를 만드세요.
    t2 = tf.constant(value=0, shape=(3,5),dtype=tf.int16)
    # 모든 원소의 값이 1인 (4,3) shape의 8-bit integer 텐서를 만드세요.
    t3 = tf.constant(value=1, shape=(4,3),dtype=tf.int8)

    return t1, t2, t3


def sequence_tensors():
    # 1.5에서 10.5까지 증가하는 3개의 텐서를 만드세요.
    seq_t1 = tf.linspace(start=1.5, stop=10.5, num=3,name=None)

    # 1에서 10까지 2씩 증가하는 텐서를 만드세요.
    seq_t2 = tf.range(start=1, limit=10, delta=2,name=None)

    return seq_t1, seq_t2


def random_tensors():
    # 난수를 생성하기 위한 seed 값입니다.
    # 정확한 채점을 위해 값을 변경하지 마세요!
    seed = 3921

    # 평균이 0이고 표준편차가 1인  정규 분포를 가진 (7,4) shape의 32-bit float 난수 텐서를 만드세요.
    # 정확한 채점을 위하여 미리 설정된 seed 값을 사용해주세요.
    rand_t1 = tf.random.normal((7,4),mean=0,stddev=1,dtype=tf.float32,seed=seed)

    # 최소값이 0이고 최대값이 3인 균등 분포를 가진 (5,4,3) shape의 32-bit float 난수 텐서를 만드세요.
    # 정확한 채점을 위하여 미리 설정된 seed 값을 사용해주세요.
    rand_t2 = tf.random.uniform((5,4,3), minval=0, maxval=3,dtype=tf.float32,seed=seed)

    return rand_t1, rand_t2


def variable_tensor():
    # 값이 100인 변수 텐서를 만드세요.
    var_tensor = tf.Variable(initial_value=100)

    return var_tensor


def main():
    # 1. constant_tensors 함수를 완성하세요.
    t1, t2, t3 = constant_tensors()

    # 2. sequence_tensors 함수를 완성하세요.
    seq_t1, seq_t2 = sequence_tensors()

    # 3. random_tensors 함수를 완성하세요.
    rand_t1, rand_t2 = random_tensors()

    # 4. variable_tensor 함수를 완성하세요.
    var_tensor = variable_tensor()

    for i in [t1, t2, t3, seq_t1, seq_t2, rand_t1, rand_t2, var_tensor]:
        print(i.numpy())


if __name__ == "__main__":
    main()
```

<br>
<br>

### ex2) tensor 연산 예제

```python
import tensorflow as tf
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

a = tf.constant(10, dtype = tf.int32)
b = tf.constant(3, dtype = tf.int32)
c = tf.constant(-10, dtype = tf.int32)
boolean = tf.constant(True, dtype = tf.bool)

# 1. 단항 연산자를 사용해보세요.
neg = tf.negative(a)
logic = tf.logical_not(boolean)
absolute = tf.abs(c)

# 2. 이항 연산자를 사용해 사칙연산을 수행해보세요.
add = tf.add(a,b)
sub = tf.subtract(a,b)
mul = tf.multiply(a,b)
div = tf.truediv(a,b)

for i in [neg, logic, absolute, add, sub, mul, div]:
    print(i.numpy())
```

<br>
<br>

### ex3) 계산기 만들기

```python
import tensorflow as tf
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


# 1. 두 실수와 연산 종류를 입력받는 함수입니다. 코드를 살펴보세요.
def insert():
    x = float(input('정수 또는 실수를 입력하세요. x : '))
    y = float(input('정수 또는 실수를 입력하세요. y : '))
    cal = input('어떤 연산을 할것인지 입력하세요. (+, -, *, /)')
    return x, y, cal


# 사칙연산 함수를 구현해보세요.
def calcul(x, y, cal):
    result = 0
    if cal == '+':

    # 더하기
        result = tf.add(x,y)

    # 빼기
    if cal == '-':
        result = tf.subtract(x,y)
    # 곱하기
    if cal == '*':
        result = tf.multiply(x,y)
    # 나누기
    if cal == '/':
        result = tf.truediv(x,y)
    return result.numpy() # numpy형식으로 반드시 변환시켜줘야한다.


def main():
    # 두 실수와 연산 종류를 입력받는 insert 함수를 호출합니다.
    x, y, cal = insert()

    # calcul 함수를 호출해 실수 사칙연산을 수행하고 결과를 출력해보세요.
    print(calcul(x, y, cal))


if __name__ == "__main__":
    main()
```

<br>
<br>
<br>
<br>

# 선형회귀 구현

## 1. 선형회귀 구현 코드

```python
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

# 채점을 위해 랜덤 시드를 고정하는 코드입니다.
# 정확한 채점을 위해 코드를 변경하지 마세요!
np.random.seed(100)


# 선형 회귀 클래스 구현
class LinearModel:
    def __init__(self):
        # 1. 가중치 초기값을 1.5의 값을 가진 변수 텐서로 설정하세요.
        self.W = tf.Variable(1.5)

        # 1. 편향 초기값을 1.5의 값을 가진 변수 텐서로 설정하세요.
        self.b = tf.Variable(1.5)

    def __call__(self, X, Y):
        # 2. W, X, b를 사용해 선형 모델을 구현하세요.
        Y = tf.add(tf.multiply(X, self.W),self.b)
        return Y


# 3. MSE 값으로 정의된 loss 함수 선언
def loss(y, pred):
    return tf.reduce_mean(tf.square(y-pred))


# gradient descent 방식으로 학습 함수 선언
def train(linear_model, x, y):
    with tf.GradientTape() as t:
        current_loss = loss(y, linear_model(x, y))

    # learning_rate 값 선언
    learning_rate = 0.001

    # gradient 값 계산
    delta_W, delta_b = t.gradient(current_loss, [linear_model.W, linear_model.b])

    # learning rate와 계산한 gradient 값을 이용하여 업데이트할 파라미터 변화 값 계산
    W_update = (learning_rate * delta_W)
    b_update = (learning_rate * delta_b)

    return W_update, b_update


def main():
    # 데이터 생성
    x_data = np.linspace(0, 10, 50)
    y_data = 4 * x_data + np.random.randn(*x_data.shape) * 4 + 3

    # 데이터 출력
    plt.scatter(x_data, y_data)
    plt.savefig('data.png')
    plt.show()

    # 선형 함수 적용
    linear_model = LinearModel()

    # epochs 값 선언
    epochs = 100

    # epoch 값만큼 모델 학습
    for epoch_count in range(epochs):

        # 선형 모델의 예측 값 저장
        y_pred_data = linear_model(x_data, y_data)

        # 예측 값과 실제 데이터 값과의 loss 함수 값 저장
        real_loss = loss(y_data, linear_model(x_data, y_data))

        # 현재의 선형 모델을 사용하여  loss 값을 줄이는 새로운 파라미터로 갱신할 파라미터 변화 값을 계산
        update_W, update_b = train(linear_model, x_data, y_data)

        # 선형 모델의 가중치와 편향을 업데이트합니다.
        linear_model.W.assign_sub(update_W)
        # linear_model.W -= update_W
        linear_model.b.assign_sub(update_b)
        # linear_model.b -= update_W

        # 20번 마다 출력 (조건문 변경 가능)
        if (epoch_count % 20 == 0):
            print(f"Epoch count {epoch_count}: Loss value: {real_loss.numpy()}")
            print('W: {}, b: {}'.format(linear_model.W.numpy(), linear_model.b.numpy()))

            fig = plt.figure()
            ax1 = fig.add_subplot(111)
            ax1.scatter(x_data, y_data)
            ax1.plot(x_data, y_pred_data, color='red')
            plt.savefig('prediction.png')
            plt.show()

if __name__ == "__main__":
    main()
```

<br>
<br>
<br>
<br>

# 딥러닝 구현하기

## 1. epoch와 batch

epoch: 한 번의 epoch는 전체 데이터 셋에 대해 한 번 학습을 완료한 상태 <br>
<br>
batch: batch(보통 mini-batch라고 표현)는 나눠진 데이터 셋을 뜻한다. <br>
iteration: epoch를 나누어서 실행하는 횟수를 의미한다.<br>
<br>
ex) <br>
총 데이터가 1000개, batch size=100<br>
1 iteration = 100개 데이터에 대해서 학습<br>
1 epoch = 1000개의 데이터를 모두 봄<br>
iteration = 1000/100 = 10<br>

<br>
<br>

## 2. 단계별 딥러닝 구현

## 1) 데이터 준비하기

```python
import tensorflow as tf
import numpy as np

data = np.random.sample((100,2))
labels = np.random.sample((100,1))

dataset = tf.data.Dataset.from_tensor_slices((data, labels)) # data와 label을 가지고 데이터를 만들겠다
dataset = dataset.batch(32) # batch를 32개 즉, 1 iteration에 32개 데이터 학습
```

<br>

## 2) 딥러닝 모델 생성 함수

```python
# 인공 신경망 모델을 만들기 위한 함수
tf.keras.models.Sequential()

# 신경망 모델의 layer 구성에 필요한 함수
tf.keras.layers.Dense(units=, activation=)
# units: 레이어 안의 node 수
# activation: 적용할 activation 함수
```

<br>

## 3) 딥러닝 모델 구축

```python
tf.model = tf.keras.models.Sequential()
tf.model.add(tf.keras.layers.Dense(10, input_dim=2, activation='sigmoid'))
tf.model.add(tf.keras.layers.Dense(10, activation='sigmoid'))
tf.model.add(tf.keras.layers.Dense(2 ,activation='sigmoid'))

tf.model.compile(loss='mean_squared_error', optimizer='SGD') # loss function과 optimizer 설정
tf.model.fit(dataset, epochs=100) # training

tf.model.evaluate(dataset_test) # 평가
predicted_labels_test = tf.model.predict(data_test) # prediction (예측한 값을 return 해준다)
```

<br>
<br>

## 3. 인공신경망 구성해보기

```python
import tensorflow as tf
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from mpl_toolkits.mplot3d import Axes3D

# 채점을 위한 코드입니다.
# 정확한 채점을 위해 코드를 수정하지 마세요!
np.random.seed(100)
tf.random.set_seed(100)

# 데이터 생성
x_data = np.linspace(0, 10, 100)
y_data = 1.5 * x_data**2 -12 * x_data + np.random.randn(*x_data.shape)*2 + 0.5


# 1. 신경망 모델 생성
tf.model = tf.keras.Sequential()
tf.model.add(tf.keras.layers.Dense(20, input_dim=1, activation='relu'))
tf.model.add(tf.keras.layers.Dense(20, activation='relu'))
tf.model.add(tf.keras.layers.Dense(1))

# 2. 모델 학습 방법 설정


# 3. 모델 학습
tf.model.compile(loss='mean_squared_error', optimizer='adam')

tf.model.fit(x_data, y_data, epochs=500,verbose=2)
#verbose 인자에는 0,1,2 값을 설정할 수 있으며, 이는 모델 학습 과정 정보를 얼마나 자세하게 출력할지를 설정합니다.

# 4. 학습된 모델을 사용하여 예측값 생성 및 저장
predictions = tf.model.predict(x_data)

# 데이터 출력
plt.scatter(x_data,y_data)
plt.savefig('data.png')

# 곡선형 분포 데이터와 예측값 출력
plt.scatter(x_data ,predictions, color='red')
plt.savefig('prediction.png')
```

<br>
<br>

## 4. 인공신경망을 이용하여 결과값 예측

```python
import tensorflow as tf
import numpy as np
import random

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import csv
import numpy as np


def read_data():
    csvreader = csv.reader(open("Advertising.csv"))

    x = []
    y = []

    next(csvreader)
    for line in csvreader:
        x_i = [float(line[1]), float(line[2]), float(line[3])]
        y_i = float(line[4])
        x.append(x_i)
        y.append(y_i)

    X = np.array(x)
    Y = np.array(y)

    return X, Y

# seed를 고정하는 코드입니다.
# 정확한 채점을 위하여 코드를 변경하지 마세요!
tf.random.set_seed(123)
np.random.seed(123)

# advertising.csv 데이터가 X와 Y에 저장됩니다.
#  X는 (200, 3) 의 shape을 가진 2차원 np.array,
#  Y는 (200,) 의 shape을 가진 1차원 np.array 입니다.

#  X는 FB, TV, Newspaper column 에 해당하는 데이터,
#  Y는 Sales column 에 해당하는 데이터가 저장됩니다.
X, Y = read_data()

# 1. 학습용 데이터와 테스트용 데이터로 분리합니다.(80:20)
x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=0)

# 2. MSE 값을 1 이하로 낮추는 모델 구현하기
# 2-1. 인공신경망 모델 구성하기
model = tf.keras.models.Sequential([
    # Input Layer
    tf.keras.layers.Dense(20, input_dim=3, activation='relu'),
    tf.keras.layers.Dense(20, activation='relu'),
    tf.keras.layers.Dense(20, activation='relu'),
    # MSE 값을 1 이하로 낮출 수 있도록 여러 층의 hidden layer를 추가해보세요.
    ##########################################################

    ##########################################################
    # Output Layer
    tf.keras.layers.Dense(1)
])

# 모델 학습 방법 설정
model.compile(loss='mean_squared_error', optimizer='adam')

# 2-2. epochs 값 설정 후 모델 학습
model.fit(x_train, y_train, epochs=1000,verbose=2)

# 학습된 신경망 모델을 사용하여 예측값 생성 및 loss 출력
predicted = model.predict(x_test)

mse_test = mean_squared_error(predicted, y_test)
print("MSE on test data: {}".format(mse_test))
```

<br>
<br>

## 5. 인공신경망을 이용한 감정 분석기

```python
import io
import matplotlib.pyplot as plt
import numpy as np
import re
import math
import tensorflow as tf
import random

import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

from sklearn.feature_extraction.text import CountVectorizer

# seed를 고정하는 코드입니다.
# 정확한 채점을 위하여 값을 변경하지 마세요!
tf.random.set_seed(123)
np.random.seed(123)

special_chars_remover = re.compile("[^\w'|_]")


# 특수 문자를 제거하는 함수입니다.
def remove_special_characters(sentence):
    return special_chars_remover.sub(' ', sentence)


# 1. /data/ratings.txt 에서 데이터를 읽어, 인공신경망 학습을 위한 두 개의 리스트를 반환합니다.
def read_data():
    sentences = []
    labels = []
    with open('rating1.txt', encoding='UTF8') as rt:
        next(rt)
        for data in rt:
            tempdata = data.replace('\n','').split('\t')
            sentences.append(remove_special_characters(tempdata[1]))
            labels.append(int(tempdata[2]))

    return sentences, labels


# 2. count_vect 함수를 완성하세요.
def count_vect(sentences, testing_sentence):
    # 테스트 문장 또한 토큰 빈도수 안에 포함되어야하기 때문에 sentences 리스트에 추가합니다.
    sentences.append(testing_sentence)


    # sentences를 카운트 벡터로 변환하세요.
    Vectorizer = CountVectorizer(min_df=1)
    # CountVectorizer는 게시물마다 등장하는 단어의 빈도수를 파악해 하나의 카운트 벡터로 만들어줍니다.
    # 인자인 min_df 는 설정한 값보다 작은 빈도수를 가진 단어는 제외하여 카운트 벡터를 생성합니다.

    vector = Vectorizer.fit_transform(sentences)
    vector = vector.toarray()

    return np.array(vector)


# ANN 함수를 완성하세요.
def ANN(vector, labels):
    # 카운트 벡터로 변환된 테스트 문장 벡터를 저장합니다.
    test = vector[-1]
    # 모델 학습 데이터에서 테스트 데이터를 제거합니다.
    vector = vector[:-1]
    # 모델 입력을 위한 형태로 변환합니다.
    test = [[test]]

    # 입력 데이터의 차원은 카운트 벡터 안의 토큰 수 입니다.
    num_voca = len(vector[0])

    # 인공 신경망 생성
    tf.model = tf.keras.models.Sequential()
    tf.model.add( tf.keras.layers.Dense(20, input_dim=num_voca, activation='relu'))
    tf.model.add(tf.keras.layers.Dense(20, activation='relu'))
    tf.model.add(tf.keras.layers.Dense(2, activation='softmax'))

    # 3. loss와 optimizer를 설정하세요.
    tf.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')

    # 학습 시작
    tf.model.fit(vector, labels, epochs=500, verbose=1)

    predict = tf.model.predict(np.array(test))

    return predict


def main():

    train_sentences, labels = read_data()

    testing_sentence = "어설픈 연기들로 몰입이 전혀 안되네요"

    bow_vect = count_vect(train_sentences, testing_sentence)
    probs = ANN(np.array(bow_vect), np.array(labels))
    # 시각화 코드입니다.
    plot_title = testing_sentence
    if len(plot_title) > 50: plot_title = plot_title[:50] + "..."
    visualize_boxplot(plot_title,
                      [probs[0][0][0], probs[0][0][1]],
                      ['Negative', 'Positive'])


def visualize_boxplot(title, values, labels):
    width = .35

    print(title)

    fig, ax = plt.subplots()
    ind = np.arange(len(values))
    rects = ax.bar(ind, values, width)
    ax.bar(ind, values, width=width)
    ax.set_xticks(ind + width / 2)
    ax.set_xticklabels(labels)

    def autolabel(rects):
        # attach some text labels
        for rect in rects:
            height = rect.get_height()
            ax.text(rect.get_x() + rect.get_width() / 2., height + 0.01, '%.2lf%%' % (height * 100), ha='center',
                    va='bottom')

    autolabel(rects)

    plt.savefig("Naversentiment.svg", format="svg")


if __name__ == "__main__":
    main()
```


<br>
<br>

## 6. Fashion MNIST 학습

```python
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import random
import os

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


# seed를 고정하는 코드입니다.
# 정확한 채점을 위하여 값을 변경하지 마세요!
np.random.seed(100)
tf.random.set_seed(100)


def ANN_classifier(x_train, y_train):
    # 1-1. 인공 신경망 분류 모델을 생성합니다.
    tf.model = tf.keras.Sequential()
    tf.model.add(tf.keras.layers.Dense(units=100, input_dim=28 * 28,activation='relu'))
    tf.model.add(tf.keras.layers.Dense(units=100, activation='relu'))
    tf.model.add(tf.keras.layers.Dense(units=100, activation='relu'))
    tf.model.add(tf.keras.layers.Dense(units=10, activation='softmax'))


    # 1-2. 모델을 학습할 loss와 optimizer를 설정합니다.
    tf.model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

    # 1-3. 모델을 학습할 epochs 값을 설정합니다.
    tf.model.fit(np.array(x_train), np.array(y_train), epochs=500)

    return tf.model


def main():
    x_train = np.loadtxt('train_images.csv', delimiter=',', dtype=np.float32)
    y_train = np.loadtxt('train_labels.csv', delimiter=',', dtype=np.float32)
    x_test = np.loadtxt('test_images.csv', delimiter=',', dtype=np.float32)
    y_test = np.loadtxt('test_labels.csv', delimiter=',', dtype=np.float32)

    # 이미지 데이터를 0~1범위의 값으로 바꾸어 줍니다.
    x_train, x_test = x_train / 255.0, x_test / 255.0

    model = ANN_classifier(x_train, y_train)

    # 학습한 모델을 test 데이터를 활용하여 평가합니다.
    loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
    print('\n- TEST 정확도 :', test_acc)

    # 임의의 3가지 test data의 이미지와 레이블값을 출력하고 예측된 레이블값 출력
    predictions = model.predict(x_test)
    rand_n = np.random.randint(100, size=3)

    for i in rand_n:
        img = x_test[i].reshape(28, 28)
        plt.imshow(img, cmap="gray")
        plt.show()
        plt.savefig("test.png")

        print("Label: ", y_test[i])
        print("Prediction: ", np.argmax(predictions[i]))


if __name__ == "__main__":
    main()
```
























































































































