---
layout: post
title: Machine Learning Basis 2
description: "Machine Learning Basis"
modified: 2020-06-28
tags: [Machine Learning]
categories: [Machine Learning]
---

# 확률의 기초
<br>

## 1. 확률의 정의
어떤 사건이 일어날 것인지 혹은 일어났는지에 대한 지식 혹은 믿음을 표현하는 방법

<br>
<br>

## 2. 확률의 연산
-> statics 참고

<br>
<br>

## 3. 조건부 확률
<br>

### 1) 조건부 확률의 정의
<br>
사건 B가 일어났을 때 A가 일어날 확률<br>
P(A|B) = P(A  교집합 B) / P(B)<br>

<br>

ex)<br>

<pre>
ex) 사건 A: 주사위에서 짝수가 나오는 사건 (A:{2.4.6})
    사건 B: 주사위에서 3보다 큰 수가 나오는 사건 (B:{4,5,6})
    P(A|B) = 3보다 큰 수가 나왔을 때 그 수가 짝수일 확률
           = P({4,6}) / P({4,5,6}) = 2/3

    P(B|A) = 짝수가 나왔을 때 그 수가 3보다 클 확률
           = P({4,6}) / P({2,4,6}) = 2/3
</pre>

<br>
<br>
<br>
<br>

## 4. 몬테카를로 방법
<br>

### 1) 몬테카를로 방법 정의

물리시간에 배웠던 빨대 던지기이다.<br>
어떤 도형이 있고, 임의로 빨대를 던져 도형안에 들어간 빨대의 개수를 셈으로서 도형의 넓이를 구할 수 있다.<br>
-> 확률을 이용하여 어떤 도형의 넓이를 구하는 방식이다.<br>

<br>
<br>

### 2) 몬테카를로 방법 코드

```python
import matplotlib.pyplot as plt
import numpy as np

def main():
    plt.figure(figsize=(5,5))

    x = []
    y = []

    N = 10000

    for i in range(N):
        x.append(np.random.rand() * 2 -1) # [0 ~ 1] 값을 -> [0 ~ 2] 값으로 -> [-1 ~ 1] 값으로 변경
        y.append(np.random.rand() * 2 -1) # [0 ~ 1] 값을 -> [0 ~ 2] 값으로 -> [-1 ~ 1] 값으로 변경
    x = np.array(x)
    y = np.array(y)
    distance_from_zero = np.sqrt(x*x+y*y) # norm 구하기
    #print(distance_from_zero)
    is_inside_circle = distance_from_zero <= 1 # norm이 1이하인 값이면 원 안에 존재
    #print(is_inside_circle)

    print("Estimated pi = %f" % (np.average(is_inside_circle) * 4))

    plt.scatter(x,y,c=is_inside_circle)
    plt.show()



if __name__ == "__main__":
    main()
```

<br>
<br>
<br>
<br>


## 5. 베이즈 법칙

빈도주의자 vs 베이즈주의자<br>
(frequentist)  (Bayesian)<br>
"동전 하나를 던졌을 때 앞면이 나올 확률은 50%이다."<br>
<br>

### 1) 빈도주의자

<br>
이 동전을 수천, 수만 번 던졌을 때<br>
그 중 앞면이 50%, 뒷면이 50%가 나온다.<br>
(사건을 계속 관찰했을 때 사건의 수가 무한해지면, 사건의 확률을 정확히 정할 수 있다.)<br>
<br>

### 2) 베이즈주의자
<br>

동전 던지기의 결과가<br>
앞면이 나올 것이라는 확신(혹은 믿음)이 50%이다.<br>
(사전 지식을 바탕으로 사건에 대한 믿음 혹은 확신으로 확률을 정의한다.)<br>
<br>
<br>
베이즈주의와 빈도주의가 일맥상 통하는 이유는<br>
베이즈주의가 사전지식을 알고 있다고 하더라도, 사전 지식은 사건을 관찰함으로써 점점 업데이트가 된다.<br>
-> 결국 확률이 어떤 곳으로 수렴하는 지 관찰하는 것은 같다.<br>

<br>
<br>


### 3) 베이즈 법칙의 유도

![image](/assets/bayes.png)


<br>
<br>

### 4) 베이즈법칙 예시

#### 암 검사 키트
<br>

암 A에 대한 테스트 키트가 있다.<br>
임의의 사람이 이 암에 걸릴 확률은 1%이다.<br>
즉, 전체 인구 중 암에 걸린 사람은 1%이다.<br>
<br>
이 암을 진단할 수 있는 키트가 있는데,<br>
암에 걸린 사람은 99%의 확률로 양성 반응이 나오고,<br>
걸리지 않은 사람은 1%의 확률로 양성 반응이 나온다.<br>
<br>
키트 검사 결과 양성 반응이 나왔다면, 암에 걸렸을 확률은?<br>
<br>
<br>
<br>
암에 걸린 사건: A<br>
키트에서 양성반응이 나온 사건: X<br>
<br>
P(A|X): 키트에서 양성반응이 나왔을 때 암에 실제로 걸렸을 확률<br>
P(X|A): 암에 걸렸을 때 키트에서 양성반응이 나올 확률<br>
P(A): 암에 걸렸을 확률<br>
P(X): 키트에서 양성반응이 나올 확률<br>
<br>
P(A|X) = (P(X|A)*P(A)) / P(X)<br>
<br>
<br>

#### 1) P(X|A)의 의미<br>
암에 걸린 사건: A<br>
키트에서 양성반응이 나온 사건: X<br>
P(X|A): 암에 걸렸을 때 키트에서 양성반응이 나올 확률<br>
<br>
<br>
P(X|A) = 0.99<br>
-> 이 암을 진단할 수 있는 키트가 있는데,<br>
   암에 걸린 사람은 99%의 확률로 양성반응이 나오고,<br>
   걸리지 않은 사람은 1%의 확률로 양성반응이 나온다.<br>
<br>
<br>

#### 2) P(A)의 의미<br>
암에 걸린 사건: A<br>
키트에서 양성반응이 나온 사건: X<br>
P(A): 암에 걸렸을 확률<br>
<br>
P(A) = 0.01<br>
-> 임의의 사람이 이 암에 걸릴 확률은 1%이다.<br>
<br>
<br>

#### 3) P(X)의 의미<br>
암에 걸린 사건: A<br>
키트에서 양성반응이 나온 사건: X<br>
P(X): 키트에서 양성반응이 나올 확률<br>
<br>
P(X) = P(X 교집합 A) + P(X 교집합 !A)   [ (! = not),   (P(X 교집합 A) = P(X|A) * P(A)) ]<br>
     = P(X|A) * P(A) + P(X|!A) * P(!A)<br>
     =  0.99  * 0.01 +  0.01  * 0.99<br>
     = 0.0198<br>
<br>
<br>
cf)<br>
P(X|!A) -> 암에 걸리지 않았는데 양성이 나올 확률 = 0.01<br>
<br>
<br>
<br>

#### 4) 결론<br>
P(A|X) = (P(X|A)*P(A)) / P(X) <br>
       = (0.99 * 0.01) / 0.0198 <br>
       = 0.5<br>
<br>
       -> 키트에서 양성 반응이 나왔는데 암에 걸렸을 확률이 50% 밖에 안 된다?<br>
       <br>
       전국민의 1%가 암에 걸린다.(키트의 정확도가 높아도 암이 휘귀해서 확률이 50%가 나온다)<br>
       암이 걸린 사람 중 양성판정이 나올 확률이 99%이다.<br>
       그런데, 암이 걸리지 않은 사람 중에서도 1%의 오류가 있다.(암이 걸리지 않음에도 0.01에 대해 양성 판정을 한다.)<br>
       즉, 양성판정을 받은 상태에서 진짜 암일지, 진짜 암이 아닐지에 대한 확률이 같다.<br>
       그러므로, 키트에서 양성 반응이 나왔음에도 암에 걸렸을 확률은 50%이다.<br>
       <br>

### 5) 유방암 진단 키트 정확도 계산 코드

```python
def main():
    sensitivity = float(input()) # 0.8
    prior_prob = float(input()) # 0.004
    false_alarm = float(input()) # 0.1

    print("%.2lf%%" % (100 * mammogram_test(sensitivity, prior_prob, false_alarm)))

def mammogram_test(sensitivity, prior_prob, false_alarm):
    p_a1_b1 = sensitivity # p(A = 1 | B = 1)

    p_b1 = prior_prob    # p(B = 1)

    p_b0 = 1 - p_b1  # p(B = 0)

    p_a1_b0 = false_alarm # p(A = 1|B = 0)

    p_a1 = p_a1_b1 * p_b1 + p_a1_b0 * p_b0    # p(A = 1)  # P(A=1∣B=0)P(B=0)+ P(A=1|B=1)P(B=1) =P(A=1∣B=1)P(B=1)=

    p_b1_a1 = (p_a1_b1 * p_b1) / p_a1# p(B = 1|A = 1)

    return p_b1_a1

if __name__ == "__main__":
    main()
```
<br>
<br>
<br>
<br>


## 6. 나이브 베이즈 분류기
<br>

### 1) 나이브 베이즈 분류기
<br>

- 분류 문제의 아주 가장 기원이 되는 알고리즘
-> 지도 학습 중 분류에 해당

<br>

### 2) 분류기의 정의
-> 주어진 데이터가 어떤 클래스에 속하는지 알아내는 방법을 자동으로 학습하는 알고리즘

<br>

### 3) 예시 (사탕 기계)

<br>

1)<br>
사탕기계 A,B가 있다.<br>
이 둘은 같은 종류의 사탕을 내놓지만, 들어 있는 사탕의 비율이 다르다.<br>

<pre>
비율   빨간색  노랑색  초록색
A        2      2       1
B        1      1       1
</pre>

#### 문제
<br>

사탕 10개를 뽑아서 빨강색 4개, 노랑색 5개, 초록색 1개를 뽑앗다면<br>
이 사탕은 어느 기계에서 뽑은 것일까?<br>
->  빨강색 4개, 노랑색 5개, 초록색 1개를 뽑았을 때 A에서 뽑았을 확률과 B에서 뽑았을 확률을 구해보자.<br>
    (베이즈 법칙을 이용한다.)<br>
    
#### 풀이

#### 1))<br>
<br>
X: 사탕 10개를 뽑아 그 결과를 관측한 사건<br>
A: 사탕 기계 A에서 사탕을 뽑은 사건<br>
B: 사탕 기계 B에서 사탕을 뽑은 사건<br>
<br>
P(A|X) 와 P(B|X)를 비교하면 어떤 것이 더 클까?<br>

#### 2))<br>
<br>

P(A|X) = (P(X|A)*P(A)) / P(X) <br>
<br>
P(A|X): 사탕을 빨4, 노5, 초1로 뽑았을 때 그게 기계 A에서 뽑았을 확률<br>
P(X|A): 기계 A에서 사탕을 뽑았을 때 사탕을 빨4, 노5, 초1로 뽑을 확률<br>
P(A): A 기계에서 사탕을 뽑을 확률<br>
P(X): 어느 기계에서 뽑든 사탕을 빨4, 노5, 초1로 뽑을 확률<br>
<br>
<br>

P(B|X) = (P(X|B)*P(B)) / P(X) <br>
-> 위 설명 참고 (A를 B로 바꾸면 된다.) <br>

<br>

#### 3))<br>
<br>

우리가 하고자 하는 것은<br>
P(A|X)와 P(B|X)을 비교하여 두 비율을 알아내고자 한다.<br>
<br>
P(A|X) = (P(X|A)*P(A)) / P(X) <br>
P(B|X) = (P(X|B)*P(B)) / P(X) <br>
<br>
P(A|X) : P(B|X) = (P(X|A)*P(A)) / P(X) : (P(X|B)*P(B)) / P(X)<br>
<br>
그러므로, P(X)는 소거 가능하다.<br>
P(A|X) : P(B|X) = (P(X|A)*P(A)) : (P(X|B)*P(B))<br>

<br>

#### 4)) 사전확률 가정하기<br>
<br>
사전확률: 데이터를 관찰하기 이전에 알고있는 사전적 확률<br>
-> A 기계보다 B기계가 조금 더 좋은 자리에 있어서 일반적으로 B기계가 잘 팔린다.<br>
P(A) = 0.4<br>
P(B) = 0.6<br>

<br>

#### 5)) Likelihood(우도)<br>
-> Likelihood(우도): 모델이 데이터를 설명하는 확률<br>
(어떤 데이터가 주어졌을 때 데이터가 얼마나 이 모델에 맞는가에대한 확률)<br>
즉, P(A|X), P(B|X)의 확률로 Likelihood가 결정된다.<br>

<br>

(가정): 사탕 기계가 매우 커서, 그 안에 있는 사탕 수의 비율은 몇 개를 꺼내도 일정하게 유지된다.

<br>

P(A|X) = (P(X|A)*P(A)) / P(X) <br>
-> P(A)는 사전확률<br>
-> P(X|A)는 A가 X를 얼마나 잘 만들어내는가? (테스트하려는 모델이 데이터를 얼마나 잘 만들어내는가)<br>
   즉, A라는 기계가 X를 얼마나 잘 뽑을 것인가?<br>

<br>
<br>

#### 6)) P(X|A), P(X|B)의 계산
<br>

<pre>
비율   빨간색  노랑색  초록색
A        2      2       1
B        1      1       1
</pre>

<br>

A에서 빨간 사탕을 꺼낼 확률 = 2/5<br>
A에서 노랑 사탕을 꺼낼 확률 = 2/5<br>
A에서 초록 사탕을 꺼낼 확률 = 1/5<br>
<br>
A에서 빨간 사탕 4개를 꺼낼 확률 = (2/5)^4<br>
A에서 노랑 사탕 5개를 꺼낼 확률 = (2/5)^5<br>
A에서 초록 사탕 1개를 꺼낼 확률 = (1/5)^1<br>
<br>
그러므로 A에서 빨간 사탕 4개, 노랑 사탕 5개, 초록 사탕 1개를 꺼낼 확률<br>
= (2/5)^4 * (2/5)^5 * (1/5)^1<br>
<br>
그러므로, <br>
P(X|A) = (2/5)^4 * (2/5)^5 * (1/5)^1<br>
P(X|B) = (1/3)^4 * (1/3)^5 * (1/3)^1<br>
<br>
<br>   
P(X|A) = (2/5)^4 * (2/5)^5 * (1/5)^1<br>
P(X|B) = (1/3)^4 * (1/3)^5 * (1/3)^1<br>
-> 위 두 식은 순서를 고려하지 않았다. <br>
그러나 순서를 고려하여 모든 빨간 사탕 4개, 노랑 사탕 5개, 초록 사탕 1개를 만들 수 있는 모든 경우의 수를 고려해야한다.<br>
<br>
그러므로<br>
P(X|A) = (2/5)^4 * (2/5)^5 * (1/5)^1 * 10<br>
P(X|B) = (1/3)^4 * (1/3)^5 * (1/3)^1 * 10<br>
을 해줘야한다.<br>
<br>
우리가 알고자 하는 것은 <br>
P(X|A) 와 P(X|B)의 비율이므로, 10은 무시 가능하다.<br>
<br>
<br>
결론,<br>
P(X|A) = (2/5)^4 * (2/5)^5 * (1/5)^1 = 5.243 * 10^-5<br>
P(X|B) = (1/3)^4 * (1/3)^5 * (1/3)^1 = 1.694 * 10^-5<br>
<br>
<br>
<br>

#### 7)) P(A|X), P(B|X)의 비율 계산
<br>

P(A|X) : P(B|X) <br>
= (P(X|A)*P(A)) / P(X) : (P(X|B)*P(B)) / P(X)<br>
= (P(X|A)*P(A)) : (P(X|B)*P(B))<br>
= 5.243 * 10^-5 * 0.4 : 1.694 * 10^-5 * 0.6<br>
= 0.674 : 0.326<br>
<br>             
-> 꺼낸 10개의 사탕은 A에서 나왔을 확률이 B에서 나왔을 확률보다 두배 더 높다.<br>
<br>     
<br>
cf) <br>
P(A|X), P(B|X) : 사후 확률<br>
P(X|A), P(X|B) : likelihood<br>
P(A), P(B): 사전확률<br>
<br>
그러므로,<br>
사후 확률 = likelihood * 사전확률<br>

<br>
<br>

### 4) 나이브 베이즈 분류기를 이용한 사탕기계 코드

```python
import re
import math
import numpy as np


def main():
    M1 = {'r': 0.7, 'g': 0.2, 'b': 0.1}  # M1 기계의 사탕 비율
    M2 = {'r': 0.3, 'g': 0.4, 'b': 0.3}  # M2 기계의 사탕 비율

    test = {'r': 4, 'g': 3, 'b': 3}

    print(naive_bayes(M1, M2, test, 0.7, 0.3))


def naive_bayes(M1, M2, test, M1_prior, M2_prior):
    pm1 = M1_prior
    pm2 = M2_prior
    px_m1L = (M1['r'])**test['r'] * (M1['g'])**test['g'] * (M1['b'])**test['b']
    px_m2L = (M2['r'])**test['r'] * (M2['g'])**test['g'] * (M2['b'])**test['b']
    pm1_x =  px_m1L * pm1
    pm2_x = px_m2L * pm2
    resultPm1_x = (pm1_x/(pm1_x + pm2_x))
    resultPm2_x = (pm2_x/(pm1_x + pm2_x))
    return [resultPm1_x, resultPm2_x]



if __name__ == "__main__":
    main()
```

<br>
<br>

### 4) Bag of Words와 감정분석
<br>

#### 1)) 감정분석 방법의 개요
<br>

사탕 기계 문제를 조금만 변형시키면 된다.<br>

<pre>
    좋아  최고  싫어  별로
긍정  3    5     1     1
부정  1    1     2     4
</pre>

<br>
사탕기계가 긍정사탕과 부정사탕이 있는데,<br>
긍정사탕에서 사탕을 뽑으면 긍정적인 사탕이 나오고<br>
부정사탕에서 사탕을 뽑으면 부정적인 사탕이 나온다<br>
<br>
즉, <br>
긍정적인 모델에서는 긍정적인 단어가 나오고<br>
부정적인 모델에서는 부정적인 단어가 나온다<br>

<br>
<br>

#### 2)) Bag of Words
<br>

Bag of Words: 텍스트를 기계가 이해할 수 있는 어떤 수단으로 만들어준다.<br>
<br>

<pre>
ex)
오늘 나는 밥을 먹었다. 어제 나는 햄버거를 먹었다.
                     |
               특수 문자 제거 (.,'," 등등을 제거)
                     |
오늘 나는 밥을 먹었다 어제 나는 햄버거를 먹었다
                     |
                  Tokenize (단어 자르기)
                     |
오늘/ 나는/ 밥을/ 먹었다/ 어제/ 나는/ 햄버거를/ 먹었다
</pre>

<br>
Bag of Words가 순서를 없애준다.<br>
어떤 단어가 가방에 몇개가 들어있다는 정보만 남는다.<br>
-> 간단한 감정분석이기 때문에 단어의 순서가 없어도 분석이 가능하다.<br>
   좀 더 고차원적인 감정분석을 할 시 순서를 고려해줘야 한다.<br>
<br>
<br>
Bag of Words를 표현하는 방법<br>
-> dictionary로 표현 (출현 횟수를 기준으로)<br>
<br>
{'오늘': 1, '나는': 2, '먹었다': 2, '햄버거를': 1, '밥을': 1, '어제': 1}<br>


<br>
<br>

#### 3)) Bag of Words 만들기 코드
<br>

```python
import re

special_chars_remover = re.compile("[^\w'|_]")

def main():
    sentence = input()
    bow = create_BOW(sentence)

    print(bow)


def create_BOW(sentence):
    bow = {}
    sentence = sentence.lower()
    sentence = remove_special_characters(sentence)
    sentence = sentence.split()
    for word in sentence:
        
        if (word in bow) and len(word) >= 1:
            bow[word] += 1
        else:
            bow[word] = 1
        
        #bow.setdefault(word,0)  # 위 코드를 setdefault를 통해 한번에 해결 가능
        #bow[word] += 1  # 처음 나오면 0으로 초기화, 두번째 나오면 그냥 1 더하기
    return bow


def remove_special_characters(sentence):
    return special_chars_remover.sub(' ', sentence)


if __name__ == "__main__":
    main()
```
<br>
<br>


### 5) 감정 분류기

<pre>
    좋아  최고  싫어  별로
긍정  3    5     1     1
부정  1    1     2     4
</pre>

<br>

사탕기계를 응용해보자.<br>
<br>
#### 1)
긍정 {'좋아': 3, '최고': 5, '싫어': 1, '별로': 1}<br>
부정 {'좋아': 1, '최고': 1, '싫어': 2, '별로': 4}<br>
<br>
ex) 긍정 기계에서 "최고" 단어가 나올 확률: 5/10<br>
ex) 부정 기계에서 "싫어" 단어가 나올 확률: 2/8<br>
<br>
#### 2)
Naive Bayes를 이용<br>
P(N|X) : P(P|X) = (P(X|N)*P(N)) / P(X) : (P(X|P)*P(P)) / P(X)<br>
(N = 부정), (P = 긍정)<br>
<br>
<br>
#### 3) 학습
<br>
ex)<br>
긍정적인 문서 2000개, 부정적인 문서 2000개를 준다.<br>
각각의 문서 셋들에서 나오는 단어의 빈도수를 측정한다.<br>
<br>
-> 학습의 목표<br>
P(N|X) : P(P|X) = (P(X|N)*P(N)) : (P(X|P)*P(P))<br>
<br>
한 문장의 단어들이 N에서 뽑힐 확률과 <br>
한 문장의 단어들이 P에서 뽑힐 확률을 비교해서 <br>
어느 비율로 N과 P에서 나올 지 계산한다.<br>
<br>
<br>
#### 4) 만약 Bag of Word에 단어가 없을 경우
-> Bag of Word에 없는 단어에 대해서는 아주 작은 숫자를 넣어 해결해준다.<br>
<br>
<br>


### 6) 감정 분류기 코드
<br>

```python
import io
import numpy as np
import matplotlib.pyplot as plt
import re
import math

special_chars_remover = re.compile("[^\w'|_]")

def remove_special_characters(sentence):
    return special_chars_remover.sub(' ', sentence)


def main():
    training_sentences = read_data()
    testing_sentence = input()
    prob_pair = naive_bayes(training_sentences, testing_sentence)
    print("부정:",prob_pair[0],"긍정:", prob_pair[1])
    plot_title = testing_sentence
    if len(plot_title) > 50: plot_title = plot_title[:50] + "..."
    visualize_boxplot(plot_title,
                      list(prob_pair),
                      ['Negative', 'Positive'])


def naive_bayes(training_sentences, testing_sentence):
    log_prob_negative = calculate_doc_prob(training_sentences[0], testing_sentence, 0.1) + math.log(0.5)
    log_prob_positive = calculate_doc_prob(training_sentences[1], testing_sentence, 0.1) + math.log(0.5)
    prob_pair = normalize_log_prob(log_prob_negative, log_prob_positive)

    return prob_pair

def read_data():
    training_sentences = [[],[]]
    line=[]
    with open('ratings.txt','rt', encoding='UTF8') as rt:
        next(rt)
        while True:
            line = rt.readline().replace('\n','')
            if not line:
                break
            else:
                line = line.split('\t')
                if line[2] == '1':
                    training_sentences[1].append(line[1])
                elif line[2] == '0':
                    training_sentences[0].append(line[1])
    return [' '.join(training_sentences[0]), ' '.join(training_sentences[1])]


def normalize_log_prob(prob1, prob2):
    '''
    숙제 4
    로그로 된 확률값을 표준화합니다.
    이 부분은 이미 작성되어 있습니다.
    '''

    maxprob = max(prob1, prob2)

    prob1 -= maxprob
    prob2 -= maxprob
    prob1 = math.exp(prob1)
    prob2 = math.exp(prob2)

    normalize_constant = 1.0 / float(prob1 + prob2)
    prob1 *= normalize_constant
    prob2 *= normalize_constant

    return (prob1, prob2)


def calculate_doc_prob(training_sentence, testing_sentence, alpha):
    logprob = 0
    sumn = 0


    training_model = create_BOW(training_sentence)
    testing_model = create_BOW(testing_sentence)

    for num in training_model.values():
        sumn += num

    for key in training_model.keys():
        training_model[key] = training_model[key] / sumn


    for key in testing_model.keys():
        if key in training_model:
            logprob += math.log(training_model[key] ** testing_model[key])
        else:
            logprob += math.log(alpha / sumn)
    '''
    숙제 3
    training_sentence로 만들어진 모델이,
    testing_sentence를 만들어 낼 **로그 확률** 을 구합니다.
    일반 숫자에서 로그값을 만들기 위해서는 math.log() 를 사용합니다.

    일반 숫자에서의 곱셈이 로그에서는 덧셈, 나눗셈은 뺄셈이 된다는 점에 유의하세요.
    예) 3 * 5 = 15
        log(3) + log(5) = log(15)

        5 / 2 = 2.5
        log(5) - log(2) = log(2.5)
    '''

    return logprob


def create_BOW(sentence):
    '''
    숙제 2
    이전 실습과 동일하게 bag of words를 만듭니다.
    '''
    bow = {}
    sentence = sentence.lower()
    sentence = remove_special_characters(sentence)
    sentence = sentence.split()
    for word in sentence:


        bow.setdefault(word,0)  # 위 코드를 setdefault를 통해 한번에 해결 가능
        bow[word] += 1  # 처음 나오면 0으로 초기화, 두번째 나오면 그냥 1 더하기
    return bow


def visualize_boxplot(title, values, labels):
    width = .35

    print(title)

    fig, ax = plt.subplots()
    ind = np.arange(len(values))
    rects = ax.bar(ind, values, width)
    ax.bar(ind, values, width=width)
    ax.set_xticks(ind + width / 2)
    ax.set_xticklabels(labels)

    def autolabel(rects):
        # attach some text labels
        for rect in rects:
            height = rect.get_height()
            ax.text(rect.get_x() + rect.get_width() / 2., height + 0.01, '%.2lf%%' % (height * 100), ha='center',
                    va='bottom')

    autolabel(rects)

    plt.savefig("image.svg", format="svg")


if __name__ == "__main__":
    main()

```





















































































