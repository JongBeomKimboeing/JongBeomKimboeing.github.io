---
layout: post
title: Machine Learning Basis 3
description: "Machine Learning Basis"
modified: 2020-06-28
tags: [Machine Learning]
categories: [Machine Learning]
---

# 비지도학습 개론

## 1. 비지도 학습
<br>

비지도학습<br>
: 라벨링이 돼 있지 않은 데이터를 학습시켜 답이 정해져 있지 않은 데이터에서 숨겨진 구조를 파악한다.<br>
-> 데이터가 가지고 있는 구조에 의해 비슷한 구조를 가지고 있는 데이터끼리 뭉친다.<br>
<br>
<br>
<br>


# hard vs soft clustering

## 1. hard clustering
<br>

hard clustering: 데이터 포인트들은 비슷한 것들끼리 뭉쳐있다.<br>
-> 섞일 수 없는 데이터를 구분<br>
ex) 고양이와 강이지 구분<br>
    구분 선을 기준으로, 100% 고양이인 영역과 100% 강아지인 영역이 나뉜다 <br>
<br>
<br>

## 2. soft clustering
<br>

soft clustering: 한 개의 데이터 포인트는 숨겨진 클러스터들의 결합니다.<br>
-> 섞일 수 있는 데이터를 구분<br>
<br>

ex) 
책 장르 (과학, 역사, 판타지 기준으로)<br>
<br>
이 책은 과학 60%, 역사 5%, 판타지 35% 의 비율로 섞여있다<br>
-> 사이언스 픽션<br>
<br>
이 책은 과학 0% 역사 45%, 판타지 55% 의 비율로 섞여있다<br>
-> 역사 판타지 <br>
<br>
**즉, 어떤 구분선으로 나눌 수 없다**

<br>
<br>

## 3. hard clustering 과  soft clustering 의 대표적인 알고리즘

### hard clustering 의 대표적인 알고리즘
1) K-Means clustering<br>
2) Hierarchical Clusering<br>
3) DBSCAN<br>
4) OPTICS<br>
<br>

### soft clustering 의 대표적인 알고리즘<br>
1) Gaussian Mixture Models (EM)<br>
2) Soft K-Means<br>
3) Topic Models<br>
4) FCM<br>
<br>

cf) 자연적으로 일어난 데이터는 soft clustering이 많이 쓰임<br>
    그러므로, 일반적으로 soft clustering을 많이 쓴다.<br>
<br>
<br>

## 4. hard clustering 의 목표

hard clustering 의 목표<br>
-> 비슷한 데이터 포인터끼리 모은다.<br>
<br>
ex) 사람이 비슷하게 뭉친 클러스터 2개(k개)를 찾아달라고 명령을 한다.<br>
-> 컴퓨터는 각각의 데이터포인트 사이의 거리를 가지고 비슷한 것 끼리 뭉치게한다.<br>
<br>
<br>

## 5. k 선택 시 고려할 것들

1) 데이터의 특성<br>
<br>

- 어떻게 만들어진 데이터인가? (데이터의 소스가 어떤 특성을 가지고 있는가?)<br>

- 데이터 포인트 외 다른 feature<br>
<br>

ex) 사람의 몸무게<br>
- 만약 데이터가 남성과 여성의 데이터로 나뉘어 져 있다. <br>
   (일반적으로 남성이 여성보다 몸무게가 많이 나가기 때문에 k=2로 설정)<br>
   
- 남성 중에서 운동선수인 사람과 평범한 사람<br>
   (일반 남성과 운동선수인 사람으로 나뉘므로 k=2)<br>
<br>
<br>

2) 분석 결과로 얻고자 하는 것<br>
<br>

- 고양이 vs 개 분류  (k=2)
- 사람들의 행동 분석
<br>
ex) 사람의 행동을 5개로 분류할 수 있다.(k=5)<br>

- 가격 대비 효율성 분석<br>

<br>
<br>
<br>
<br>

# PCA 차원 축소법

## 1. 차원축소
데이터의 특성이나 데이터에대한 사전지식이 없을 때, k를 결정하는 것을 도와준다.
<br>
<br>

ex)<br>
내가 와이너리의 데이터 사이언티스트이다.<br>
지금까지 와이너리 회사에서 수백 가지의 와인을 생산했고,<br>
생산팀은 그 와인들에 대한 13가지 특성을 측정해서 정리했다.<br>
<br>
**문제**
지금까지 생산한 와인들을 종류별로 모아서 라인업으로 만들고 싶다.<br>
178개의 와인들을 몇 가지로 분류할 수 있을까?<br>
<br>

-> k를 결정하는 데 있어서 가장 쉽게 결정하는 방법은 눈으로 데이터를 봐서 k를 결정하는 것이다.<br>
그러나, 현재 데이터는 13가지 특성으로 나뉘어 있다.<br>
13가지 특성으로 나뉘어 있는 현재 데이터를 시각화 하면, 13차원의 그래프로 나타내어진다.<br>
13차원은 우리가 이해할 수 없다.<br>
<br>
그러므로, 우리는 최소한 이 그래프를 2차원 또는 3차원으로 줄여야 눈으로 데이터를 확인하고 이해 가능하다.<br>
-> 이렇게 차원을 줄이기 위해 사용하는 것이 PCA이다.<br>
<br>
<br>

## 2. 차원축소 목표
어떤 고차원의 데이터를 2차원/3차원으로 줄여서 데이터들을 2차원/3차원 형태로 보는 것이 목표이다.<br>
-> 이를 통해 클러스팅을 할 때 k를 몇으로 할 지 정해줄 수 있다.<br>
<br>
<br>

## 3. 주성분 분석 (PCA)

***PCA (Principal Component Analysis) (주성분 분석)***

###PCA 를 왜 사용하는가?
1. 고차원 데이터를 저차원으로 줄이기 위해 (시각화 목적)

2. 고차원에서 저차원으로 줄일 때 발생하는 데이터 손실을 최소화하는 것이 목표이다.

3. 데이터 정제를 위해 사용
-> 3차원 데이터인데, 데이터들이 2차원 평면을 이루고 있는 경우 3차원 데이터들을 2차원으로 투영시킨다.

   3차원 데이터가 2차원 평면을 이루고 있는 경우는 주로, 데이터를 수집할 때 발생한 noise에 의한 것이다.
   만약, 이 데이터가 사실은 2차원 데이터에서 나온 것이 확실하다면 이것을 3차원에서 2차원으로 줄임으로써
   쓸때 없는 noise를 제거할 수 있다.
   

PCA 는 주로 데이터 정제에 많이 사용된다.
왜냐하면 k-means 나 여러 clustering 방법은 단지 데이터간의 거리를 측정하기만 하면 clustering 을 할 수 있어서
데이터의 차원에 영향을 받지 않는다.

차원의 영향을 받지 않음에도 불구하고 PCA를 사용하는 이유는
데이터의 noise 를 줄여 데이터 정제를 하기 위해서이다.

차원이 높다는 것은 데이터를 표현할 수 있는 자유도가 높다는 의미이다.
자유도가 너무 높을 경우 모듈이 제대로 작동하지 않는다.
그렇기 때문에 적당한 차원으로 줄여 자유도를 제한할 필요가 있다.



















































































































