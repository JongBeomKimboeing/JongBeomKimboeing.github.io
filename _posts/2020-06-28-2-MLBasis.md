---
layout: post
title: Machine Learning Basis 2
description: "Machine Learning Basis"
modified: 2020-06-28
tags: [Machine Learning]
categories: [Machine Learning]
---

# 확률의 기초
<br>

## 1. 확률의 정의
어떤 사건이 일어날 것인지 혹은 일어났는지에 대한 지식 혹은 믿음을 표현하는 방법

<br>
<br>

## 2. 확률의 연산
-> statics 참고

<br>
<br>

## 3. 조건부 확률
<br>

### 1) 조건부 확률의 정의
<br>
사건 B가 일어났을 때 A가 일어날 확률<br>
P(A|B) = P(A  교집합 B) / P(B)<br>

<br>

ex)<br>

<pre>
ex) 사건 A: 주사위에서 짝수가 나오는 사건 (A:{2.4.6})
    사건 B: 주사위에서 3보다 큰 수가 나오는 사건 (B:{4,5,6})
    P(A|B) = 3보다 큰 수가 나왔을 때 그 수가 짝수일 확률
           = P({4,6}) / P({4,5,6}) = 2/3

    P(B|A) = 짝수가 나왔을 때 그 수가 3보다 클 확률
           = P({4,6}) / P({2,4,6}) = 2/3
</pre>

<br>
<br>

## 4. 몬테카를로 방법
<br>

### 1) 몬테카를로 방법 정의

물리시간에 배웠던 빨대 던지기이다.<br>
어떤 도형이 있고, 임의로 빨대를 던져 도형안에 들어간 빨대의 개수를 셈으로서 도형의 넓이를 구할 수 있다.<br>
-> 확률을 이용하여 어떤 도형의 넓이를 구하는 방식이다.<br>

<br>
<br>

### 2) 몬테카를로 방법 코드

```python
import matplotlib.pyplot as plt
import numpy as np

def main():
    plt.figure(figsize=(5,5))

    x = []
    y = []

    N = 10000

    for i in range(N):
        x.append(np.random.rand() * 2 -1) # [0 ~ 1] 값을 -> [0 ~ 2] 값으로 -> [-1 ~ 1] 값으로 변경
        y.append(np.random.rand() * 2 -1) # [0 ~ 1] 값을 -> [0 ~ 2] 값으로 -> [-1 ~ 1] 값으로 변경
    x = np.array(x)
    y = np.array(y)
    distance_from_zero = np.sqrt(x*x+y*y) # norm 구하기
    #print(distance_from_zero)
    is_inside_circle = distance_from_zero <= 1 # norm이 1이하인 값이면 원 안에 존재
    #print(is_inside_circle)

    print("Estimated pi = %f" % (np.average(is_inside_circle) * 4))

    plt.scatter(x,y,c=is_inside_circle)
    plt.show()



if __name__ == "__main__":
    main()
```

<br>
<br>


## 5. 베이즈 법칙

빈도주의자 vs 베이즈주의자<br>
(frequentist)  (Bayesian)<br>
"동전 하나를 던졌을 때 앞면이 나올 확률은 50%이다."<br>
<br>

### 1) 빈도주의자

<br>
이 동전을 수천, 수만 번 던졌을 때<br>
그 중 앞면이 50%, 뒷면이 50%가 나온다.<br>
(사건을 계속 관찰했을 때 사건의 수가 무한해지면, 사건의 확률을 정확히 정할 수 있다.)<br>
<br>

### 2) 베이즈주의자
<br>

동전 던지기의 결과가<br>
앞면이 나올 것이라는 확신(혹은 믿음)이 50%이다.<br>
(사전 지식을 바탕으로 사건에 대한 믿음 혹은 확신으로 확률을 정의한다.)<br>
<br>
<br>
베이즈주의와 빈도주의가 일맥상 통하는 이유는<br>
베이즈주의가 사전지식을 알고 있다고 하더라도, 사전 지식은 사건을 관찰함으로써 점점 업데이트가 된다.<br>
-> 결국 확률이 어떤 곳으로 수렴하는 지 관찰하는 것은 같다.<br>

<br>
<br>


### 3) 베이즈 법칙의 유도

![image](/assets/bayes.png)


<br>
<br>

### 4) 베이즈법칙 예시

#### 암 검사 키트
<br>

암 A에 대한 테스트 키트가 있다.<br>
임의의 사람이 이 암에 걸릴 확률은 1%이다.<br>
즉, 전체 인구 중 암에 걸린 사람은 1%이다.<br>
<br>
이 암을 진단할 수 있는 키트가 있는데,<br>
암에 걸린 사람은 99%의 확률로 양성 반응이 나오고,<br>
걸리지 않은 사람은 1%의 확률로 양성 반응이 나온다.<br>
<br>
키트 검사 결과 양성 반응이 나왔다면, 암에 걸렸을 확률은?<br>
<br>
<br>
<br>
암에 걸린 사건: A<br>
키트에서 양성반응이 나온 사건: X<br>
<br>
P(A|X): 키트에서 양성반응이 나왔을 때 암에 실제로 걸렸을 확률<br>
P(X|A): 암에 걸렸을 때 키트에서 양성반응이 나올 확률<br>
P(A): 암에 걸렸을 확률<br>
P(X): 키트에서 양성반응이 나올 확률<br>
<br>
P(A|X) = (P(X|A)*P(A)) / P(X)<br>
<br>
<br>

#### 1) P(X|A)의 의미<br>
암에 걸린 사건: A<br>
키트에서 양성반응이 나온 사건: X<br>
P(X|A): 암에 걸렸을 때 키트에서 양성반응이 나올 확률<br>
<br>
<br>
P(X|A) = 0.99<br>
-> 이 암을 진단할 수 있는 키트가 있는데,<br>
   암에 걸린 사람은 99%의 확률로 양성반응이 나오고,<br>
   걸리지 않은 사람은 1%의 확률로 양성반응이 나온다.<br>
<br>
<br>

#### 2) P(A)의 의미<br>
암에 걸린 사건: A<br>
키트에서 양성반응이 나온 사건: X<br>
P(A): 암에 걸렸을 확률<br>
<br>
P(A) = 0.01<br>
-> 임의의 사람이 이 암에 걸릴 확률은 1%이다.<br>
<br>
<br>

#### 3) P(X)의 의미<br>
암에 걸린 사건: A<br>
키트에서 양성반응이 나온 사건: X<br>
P(X): 키트에서 양성반응이 나올 확률<br>
<br>
P(X) = P(X 교집합 A) + P(X 교집합 !A)   (! = not)  (P(X 교집합 A) = P(X|A) * P(A))<br>
     = P(X|A) * P(A) + P(X|!A) * P(!A)<br>
     =  0.99  * 0.01 +  0.01  * 0.99<br>
     = 0.0198<br>
<br>
<br>
cf)<br>
P(X|!A) -> 암에 걸리지 않았는데 양성이 나올 확률 = 0.01<br>
<br>
<br>
<br>

#### 4) 결론<br>
P(A|X) = (P(X|A)*P(A)) / P(X) <br>
       = (0.99 * 0.01) / 0.0198 <br>
       = 0.5<br>
<br>
       -> 키트에서 양성 반응이 나왔는데 암에 걸렸을 확률이 50% 밖에 안 된다?<br>
       <br>
       전국민의 1%가 암에 걸린다.(키트의 정확도가 높아도 암이 휘귀해서 확률이 50%가 나온다)<br>
       암이 걸린 사람 중 양성판정이 나올 확률이 99%이다.<br>
       그런데, 암이 걸리지 않은 사람 중에서도 1%의 오류가 있다.(암이 걸리지 않음에도 0.01에 대해 양성 판정을 한다.)<br>
       즉, 양성판정을 받은 상태에서 진짜 암일지, 진짜 암이 아닐지에 대한 확률이 같다.<br>
       그러므로, 키트에서 양성 반응이 나왔음에도 암에 걸렸을 확률은 50%이다.<br>
       <br>



















































































































